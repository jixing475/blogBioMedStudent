[
  {
    "path": "posts/2021-06-24-ml-review/",
    "title": "Machine Learning review with an intro to the tidymodels package",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Jixing Liu",
        "url": "https://emitanaka.org"
      }
    ],
    "date": "2021-06-24",
    "categories": [
      "machine learning",
      "R"
    ],
    "contents": "\n\nContents\nFollow along\nResources\nReview\nUsing tidymodels for the process\n1. Quick exploration\n2. Data splitting\n3. Data pre-processing\n4. Fitting model(s)\n6. Evaluate & compare models\n7. Apply model to testing data\n9. Question the data and model\n8. Use the model\n5. Tuning model parameters\n\nRef\n\n\n\n\n\n\n\n\n\n\n\nFollow along\nYou can download this .Rmd file below if you’d like to follow along. I do have a few hidden notes you can disregard. This document is a distill_article, so you may want to change to an html_document to knit. You will also need to delete any image references to properly knit, since you won’t have those images.\n\n\n\n Download .Rmd file\n\n\n\nResources\nHere are some great resources. I will reference some of them throughout.\nHands on Machine Learning with R (HOML, for short) by Bradley Boehmke and Brandon Greenwell is the textbook I used in the Machine Learning course I taught in spring of 2020. It is a great place to go to review some of the model algorithms and concepts.\nISLR by James, Witten, Hastie, and Tibshirani goes deeper into the math of the algorithms. You can download their book at this site.\nTidymodels\nLisa’s tidymodels noRth presentation gives an example of using tidymodels. I will go through the same example below, with a few added parts. The code has changed slightly in some places, too.\ntidymodels.org, specifically the case study, walks through examples of using the tidymodels suite.\nTidy Models with R textbook by Julia Silge and Max Kuhn provides more in-depth explanations of the tidymodels functions with extended examples.\nJulia Silge’s blog with even more examples!\n\nReview\nMost of you probably learned about machine learning algorithms using the caret R package. Before jumping into the new tidymodels package, let’s remember some of the key machine learning concepts.\nLet’s start with an overview of the process. You covered many of these in your machine learning course. If you need more of a refresher than what I provide, see the Modeling Process chapter of HOML.\n\nHands-On Machine Learning with R\n这是一本很棒的讲机器学习的书籍\n\nimage-20210624143944804\n\n\nimage-20210624143407974And let’s review what we do during each of these steps.\nQuick exploration: Read in the data, check variable types, find which values each variable takes and how often, check distributions of quantitative variables, explore missing values. 📌 DO NOT do any modeling or transforming of data in this step(现在还不是做数据转换的时候).\nData splitting: Split the data into training and testing sets. The testing dataset will not be used again until the very end.\nData pre-processing: More in-depth data exploration, feature engineering, variable transformations. This step is usually pretty time-consuming.\nFitting model(s): Fit the models of interest on the training data.\nTuning parameters: If the model in the previous step involved tuning parameters, use cross-validation (or similar method) to find the best parameter.\nEvaluate & compare models: Use cross-validation to evaluate the model. If you have a large number of models you are evaluating, you will probably limit the set of models to your best/favorite few during this step. The image below is to help you remember that process, which I have also written about below.\nimage-20210624144138235In \\(k\\)-fold cross-validation, we divide the data randomly into \\(k\\) approximately equal groups or folds. The schematic here shows 5-fold cross-validation.\nThe model is fit on \\(k-1\\) of the folds and the remaining fold is used to evaluate the model. Let’s look at the first row in the schematic. Here the model is fit on the data that are in folds 2, 3, 4, and 5. The model is evaluated on the data in fold 1.\nRMSE is a common performance metric for models with a quantitative response. It is computed by taking the difference between the predicted and actual response for each observation, squaring it, and taking the square root of the average over all observations. Or, as a formula:\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i)^2},\n\\]\nSo, again looking at the first row in the schematic, the model is fit to folds 2, 3, 4, and 5 and we would use that model to compute the RMSE for fold 1. In the second row, the model is fit to the data in folds 1, 3, 4, and 5 and that model is used to compute the RMSE for the data in the 2nd fold.\nAfter this is done for all 5 folds, we take the average RMSE, to obtain the overall performance. This overall error is sometimes called the CV error. Averaging the performance over \\(k\\) folds gives a better estimate of the true error than using one hold-out set. It also allows us to estimate its variability.\nFor models with a categorical response, a common performance metric to evaluate a model is accuracy: out of all cases, the fraction of correct (true positives and true negatives) classifications. A cross-validated accuracy would be computed in a similar way to the cross-validated RMSE described above.\nApply final few models to testing data: After we limit the number of models to the top few, we will want to to apply it to the testing data, the data that hasn’t been used at all during the modeling process. This will give us a measure of the model’s performance and may help us make a final decision about which model to use.\nUse the model or model deploy !: This step may be simple, like applying the model to a single set of data, or it could be a lot more complex, requiring the model to be “put into production” so it can be applied to new data in real-time.\nQuestion the data and model: This isn’t really a single step but something that we should be doing through the modeling process. We should be working closely with people who know the data well so we assure that we are interpreting and using it correctly. And we should evaluate how the model might be used in new contexts, especially keeping in mind how the model could be used to do harm.\nUsing tidymodels for the process\nIn this section, I will show how we can use the tidymodels framework to execute the modeling process. I’ve updated the diagram from above to include some of the libraries and functions we’ll use throughout the process.\n\nimage-20210624144525718image-20210624145832107\nFirst, let’s load some of the libraries we will use:\n\n\n\nlibrary(tidyverse)         # for reading in data, graphing, and cleaning\nlibrary(tidymodels)        # for modeling ... tidily\nlibrary(glmnet)            # for regularized regression, including LASSO\nlibrary(naniar)            # for examining missing values (NAs)\nlibrary(lubridate)         # for date manipulation\nlibrary(moderndive)        # for King County housing data\nlibrary(vip)               # for variable importance plots\nlibrary(rmarkdown)         # for paged tables\ntheme_set(theme_minimal()) # my favorite ggplot2 theme :)\n\n\n\n\nRead in the King County Housing data and take a look at the first 5 rows.\n\n\n\ndata(\"house_prices\")\n\nhouse_prices %>% \n  slice(1:5)\n\n\n# A tibble: 5 x 21\n  id         date        price bedrooms bathrooms sqft_living sqft_lot\n  <chr>      <date>      <dbl>    <int>     <dbl>       <int>    <int>\n1 7129300520 2014-10-13 221900        3      1           1180     5650\n2 6414100192 2014-12-09 538000        3      2.25        2570     7242\n3 5631500400 2015-02-25 180000        2      1            770    10000\n4 2487200875 2014-12-09 604000        4      3           1960     5000\n5 1954400510 2015-02-18 510000        3      2           1680     8080\n# … with 14 more variables: floors <dbl>, waterfront <lgl>,\n#   view <int>, condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>\n\n\nNow, we will dig into each of the modeling steps listed above.\n1. Quick exploration\nTake a quick look at distributions of all the variables to check for anything irregular.\nQuantitative variables:\n\n\n\n# house_prices %>% \n#   select(where(is.numeric)) %>% \n#   pivot_longer(cols = everything(),\n#                names_to = \"variable\", \n#                values_to = \"value\") %>% \n#   ggplot(aes(x = value)) +\n#   geom_histogram(bins = 30) +\n#   facet_wrap(vars(variable), \n#              scales = \"free\")\n\nhouse_prices %>% \n  select(where(is.numeric)) %>% \n  DataExplorer::plot_histogram(theme_config = theme_minimal())\n\n\n\n\nThings I noticed and pre-processing thoughts:\nRight-skewness in price and all variables regarding square footage –> log transform if using linear regression.\nMany 0’s in sqft_basement, view, and yr_renovated –> create indicator variables of having that feature vs. not, ie. a variable called basement where a 0 indicates no basement (sqft_basement = 0) and a 1 indicates a basement (sqft_basement > 0).\nAge of home may be a better, more interpretable variable than year built –> age_at_sale = year(date) - yr_built.\n\nCategorical variables:\n\n\n\n# house_prices %>% \n#   select(where(is.factor)) %>% \n#   pivot_longer(cols = everything(),\n#                names_to = \"variable\", \n#                values_to = \"value\") %>% \n#   ggplot(aes(x = value)) +\n#   geom_bar() +\n#   facet_wrap(vars(variable), \n#              scales = \"free\", \n#              nrow = 2)\n\nhouse_prices %>%\n  select(where(is.factor)) %>%\n  DataExplorer::plot_bar()\n\n\n\n\nThings I noticed and pre-processing thoughts:\ncondition and grade both have levels with low counts –> make fewer categories. (这俩变量低频率的类别有很多, 要精简一下)\nzipcode has many unique levels –> don’t use that variable for now. (这个基本上都是 unique 的值, 不适合用做因子变量)\nWe might consider using the month the house was sold as a variable:\n\n\n\nhouse_prices %>% \n  count(month = month(date, label = TRUE)) %>% \n  ggplot() +\n  geom_col(aes(x = month, y = n))\n\n\n\n\nAnd, we quickly look at the counts for the waterfront variable. Not many houses are waterfront properties.\n\n\n\nhouse_prices %>% \n  count(waterfront)\n\n\n# A tibble: 2 x 2\n  waterfront     n\n  <lgl>      <int>\n1 FALSE      21450\n2 TRUE         163\n\n\nThe only other variable is id which isn’t used in modeling.\nBefore moving on, let’s use the add_n_miss()(对行进行缺失值计算统计) function from the naniar library to see if we have any missing values. And it appears that there aren’t any missing values - lucky us!\n\n\n\nhouse_prices %>% \n  add_n_miss() %>% \n  count(n_miss_all)\n\n\n# A tibble: 1 x 2\n  n_miss_all     n\n       <int> <int>\n1          0 21613\n\n\n2. Data splitting\nNOTE: I start by doing some manipulating of the dataset to use log_price as the response variable rather than price. I originally did this using a step_log() function after a recipe() function (see the next section), but read in this RStudio Community post, in the comment by Max Kuhn, that it’s better to transform the outcome before doing the modeling. There is also a discussion of this in the Skipping steps for new data section of the Kuhn & Silge Tidy Modeling with R book.\n\n\n\nThen, we split the data into: training + testing datasets.\nWe use the training data to fit different types of models and to tune parameters of those models if needed, and to help us choose our best models (or maybe best few).\nThe testing dataset is saved for the very end to compare a small subset of models and to give an estimate of the performance on data the model hasn’t seen.\nThe initial_split() function from the rsample library (part of tidymodels) is used to create this split. We just do random splitting with this dataset, but there are other arguments that allow you to do stratified sampling. Then we use training() and testing() to extract the two datasets, house_training and house_testing.\n\n\n\nset.seed(327) #for reproducibility\n\nhouse_prices <- house_prices %>% \n  mutate(log_price = log(price, base = 10)) %>% \n  select(-price)\n\n# Randomly assigns 75% of the data to training.\nhouse_split <- initial_split(house_prices, \n                             prop = .75)\nhouse_split\n\n\n<Analysis/Assess/Total>\n<16209/5404/21613>\n\n#<training/testing/total>\n\nhouse_training <- training(house_split)\nhouse_testing <- testing(house_split)\n\n\n\n\n3. Data pre-processing\nThis step may not seem very time consuming in this example, but you will often come back to this step and spend a lot of time trying different variable transformations. You should make sure to work closely with the people who use and create the data during this step. They are a crucial part of the process.\nWe use the recipe() function to define the response/outcome variable and the predictor variables.\nA variety of step_xxx() functions can be used to do any data pre-processing/transforming. Find them all here. I used a few, with brief descriptions in the code. I also used some selector functions, like all_predictors() and all_nominal() to help me select the right variables. For suggestions on which step_xxx() functions to use with which engines, see this list of functions with 📌 example templates. (Thank you Alison Hill, for showing me this resource via R Studio Community.)\nOne thing that is different here from what you might be used to is that we explicitly make dummy variables with the step_dummy() function. We don’t need to always do that - it depends on the model we use(不是所有的模型都要求转换哑变量). The 📌 recommended preprocessing appendix of the Tidy Modeling with R book is a good place to look for reference.\nWe also use step_normalize() here to center and scale our numeric predictor variables. Even though this is the default in the glmnet engine (see next step), 🔥 this is important to ensure that this step happens with new data.\nThe update_roles() function is used to change the roles of some variables. For us, these are variables we may want to include for evaluation purposes but will not be used in building the model. I chose the role of evaluative but you could name that role anything you want, eg. id, extra, junk (maybe a bad idea?).\n\n\n\nhouse_recipe <- recipe(log_price ~ ., #short-cut, . = all other vars\n                       data = house_training) %>% \n  # Pre-processing:\n  # Remove, redundant to sqft_living and sqft_lot\n  step_rm(sqft_living15, sqft_lot15) %>%\n  # log sqft variables (without price)\n  step_log(starts_with(\"sqft\"),\n           -sqft_basement, \n           base = 10) %>% \n  # I originally had the step_log() function below\n  # but instead did the transformation before\n  # the recipe because this will mess up the \n  # predict() function\n  # step_log(price, base = 10) %>% \n  # 📝在 recipes 开始之前, 对 Y 进行 step_log 这样对 predict() function\n  # 的预测值不会造成干扰\n  \n  # new grade variable combines low grades & high grades\n  # indicator variables for basement, renovate, and view \n  # waterfront to numeric\n  # age of house\n  step_mutate(grade = as.character(grade),\n              grade = fct_relevel(\n                        case_when(\n                          grade %in% \"1\":\"6\"   ~ \"below_average\",\n                          grade %in% \"10\":\"13\" ~ \"high\",\n                          TRUE ~ grade\n                        ),\n                        \"below_average\",\"7\",\"8\",\"9\",\"high\"),\n              basement = as.numeric(sqft_basement == 0),\n              renovated = as.numeric(yr_renovated == 0),\n              view = as.numeric(view == 0),\n              waterfront = as.numeric(waterfront),\n              age_at_sale = year(date) - yr_built)%>% \n  # Remove sqft_basement, yr_renovated, and yr_built\n  step_rm(sqft_basement, \n          yr_renovated, \n          yr_built) %>% \n  # Create a month variable\n  step_date(date, \n            features = \"month\") %>% \n  # Make these evaluative variables, not included in modeling\n  update_role(all_of(c(\"id\",\n                       \"date\",\n                       \"zipcode\", \n                       \"lat\", \n                       \"long\")),\n              new_role = \"evaluative\") %>% \n  # Create indicator variables for factors/character/nominal\n  # explicitly remove outcome, even though outcome isn't nominal\n  # this is important in cases when we have a nominal output (eg. logistic)\n  step_dummy(all_nominal(), \n             -all_outcomes(), \n             -has_role(match = \"evaluative\")) %>% \n  step_normalize(all_predictors(), \n                 -all_nominal())\n\n\n\n\nApply to training dataset, just to see what happens. This is not a necessary step, 📝 but I often like to check to see that everything is as expected.\nFor example, notice the names of the variables are the same as before but they have been transformed, eg. sqft_living is actually log base 10 of square feet of living. This confused me the first time, so I was glad I ran this extra step. Better to be confused now than later in the process 😀.\n\n\n\ntraining_data_prepared <- \nhouse_recipe %>% \n  prep(house_training) %>%\n  # using bake(new_data = NULL) gives same result as juice()\n  # bake(new_data = NULL)\n  juice() \n\ntraining_data_prepared %>% \n  plot_histogram()\n\n\n\n\n4. Fitting model(s)\nNow that we have split and pre-processed the data, we are ready to model! First, we will model log_price using simple linear regression.\nWe will do this using some modeling functions from the parsnip package. Find all available functions 🔍Explore tidymodels - Search parsnip models. 📝 Here is the detail for linear regression.\nIn order to define our model, we need to do these steps:\nDefine the model type, which is the general type of model you want to fit(选着模型).\nSet the engine, which defines the package/function that will be used to fit the model(选着使用哪个 📦).\nSet the mode, which is either “regression” for continuous response variables or “classification” for binary/categorical response variables. (Note that for linear regression, it can only be “regression”, so we don’t NEED this step in this case.) 遗憾的是, 目前并不包含想生存分析这种包含时间的类型\n(OPTIONAL) Set arguments to tune. We’ll see an example of this later.\n\n\n\nhouse_linear_mod <- \n  # Define a linear regression model\n  linear_reg() %>% \n  # Set the engine to \"lm\" (lm() function is used to fit model)\n  set_engine(\"lm\") %>% \n  # Not necessary here, but good to remember for other models\n  set_mode(\"regression\")\n\nhouse_linear_mod\n\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThis is just setting up the process. We haven’t fit the model to data yet, and there’s still one more step before we do - creating a workflow! This combines the preprocessing and model definition steps.\n\n\n\nhouse_lm_wf <- \n  # Set up the workflow\n  workflow() %>% \n  # Add the recipe\n  add_recipe(house_recipe) %>% \n  # Add the modeling\n  add_model(house_linear_mod)\n# 这里有点像是 scikit-learn 里面的创建模型对象, 只是这个包含了 rawdata input ➡ data processing  ➡  ready to fit data\nhouse_lm_wf\n\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_rm()\n• step_log()\n• step_mutate()\n• step_rm()\n• step_date()\n• step_dummy()\n• step_normalize()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow we are finally ready to fit the model! After all that work, this part seems easy. We first use the fit() function to fit the model, telling it which data set we want to fit the model to. Then we use some other functions to display the results nicely.\n\n\n\nhouse_lm_fit <- \n  # Tell it the workflow\n  house_lm_wf %>% \n  # Fit the model to the training data\n  fit(house_training)\n\n# Display the results nicely\nhouse_lm_fit %>% \n  pull_workflow_fit() %>% \n  tidy() %>% \n  mutate(across(where(is.numeric), ~round(.x,3))) \n\n\n# A tibble: 31 x 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)    5.67      0.001   5345.         0\n 2 bedrooms      -0.015     0.001    -10.8        0\n 3 bathrooms      0.026     0.002     13.3        0\n 4 sqft_living    0.053     0.005     11.3        0\n 5 sqft_lot      -0.014     0.001    -10.7        0\n 6 floors         0.013     0.002      7.64       0\n 7 waterfront     0.017     0.001     15.1        0\n 8 view          -0.017     0.001    -14.8        0\n 9 sqft_above     0.026     0.005      5.66       0\n10 basement      -0.02      0.002     -8.90       0\n# … with 21 more rows\n\n\n6. Evaluate & compare models\n(I realize we skipped #5. Don’t worry, we’ll get to it.)\nTo evaluate the model, we will use cross-validation (CV), specifically 5-fold CV.\nFirst, we set up the five folds of the training data using the vfold_cv() function.\n\n\n\nset.seed(1211) # for reproducibility\nhouse_cv <- vfold_cv(house_training, v = 5)\n\n\n\n\nThen, we fit the model using the 5-fold dataset we just created (When I first did this, I thought I wouldn’t have to do both the previous step of fitting a model on the training data AND this step, but I couldn’t figure out how to extract the final model from the CV data … so this was my solution at the time … and it turns out you DO need to do both as noted by Julia Silge in this 📌 tidymodels: fastest way to extract a model object from fit_resamples() results - Machine Learning and Modeling - RStudio Community.\n\n\n\nset.seed(456) # For reproducibility - not needed for this algorithm\n\nhouse_lm_fit_cv <-\n  # Tell it the workflow\n  house_lm_wf %>% \n  # 📝  Fit the model (using the workflow) to the cv data: fit_resamples \n  fit_resamples(house_cv)\n\n# The evaluation metrics for each fold:\nhouse_lm_fit_cv %>% \n  select(id, .metrics) %>% \n  unnest(.metrics) \n\n\n# A tibble: 10 x 5\n   id    .metric .estimator .estimate .config             \n   <chr> <chr>   <chr>          <dbl> <chr>               \n 1 Fold1 rmse    standard       0.136 Preprocessor1_Model1\n 2 Fold1 rsq     standard       0.625 Preprocessor1_Model1\n 3 Fold2 rmse    standard       0.135 Preprocessor1_Model1\n 4 Fold2 rsq     standard       0.654 Preprocessor1_Model1\n 5 Fold3 rmse    standard       0.131 Preprocessor1_Model1\n 6 Fold3 rsq     standard       0.674 Preprocessor1_Model1\n 7 Fold4 rmse    standard       0.137 Preprocessor1_Model1\n 8 Fold4 rsq     standard       0.645 Preprocessor1_Model1\n 9 Fold5 rmse    standard       0.136 Preprocessor1_Model1\n10 Fold5 rsq     standard       0.644 Preprocessor1_Model1\n\n# Evaluation metrics averaged over all folds:\ncollect_metrics(house_lm_fit_cv)\n\n\n# A tibble: 2 x 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.135     5 0.00105 Preprocessor1_Model1\n2 rsq     standard   0.648     5 0.00793 Preprocessor1_Model1\n\n# Just to show you where the averages come from:\nhouse_lm_fit_cv %>% \n  select(id, .metrics) %>% \n  unnest(.metrics) %>% \n  group_by(.metric, .estimator) %>% \n  summarize(mean = mean(.estimate),\n            n = n(),\n            std_err = sd(.estimate)/sqrt(n))\n\n\n# A tibble: 2 x 5\n# Groups:   .metric [2]\n  .metric .estimator  mean     n std_err\n  <chr>   <chr>      <dbl> <int>   <dbl>\n1 rmse    standard   0.135     5 0.00105\n2 rsq     standard   0.648     5 0.00793\n\n\n7. Apply model to testing data\nIn this simple scenario, we may be interested in seeing how the model performs on the testing data that was left out. The code below will fit the model to the training data and apply it to the testing data. There are other ways we could have done this, but the way we do it here will be useful when we start using more complex models where we need to tune model parameters.\nAfter the model is fit and applied, we collect the performance metrics and display them and show the predictions from the testing data. Notice the the RMSE here is very similar to the cross-validated RMSE we computed in the previous step.\n\n\n\nhouse_lm_test <- \n  # The modeling work flow\n  house_lm_wf %>% \n  # Use training data to fit the model and apply it to testing data\n  last_fit(house_split)\n\n# performance metrics from testing data\ncollect_metrics(house_lm_test)\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.136 Preprocessor1_Model1\n2 rsq     standard       0.654 Preprocessor1_Model1\n\n# predictions from testing data\ncollect_predictions(house_lm_test) \n\n\n# A tibble: 5,404 x 5\n   id               .pred  .row log_price .config             \n   <chr>            <dbl> <int>     <dbl> <chr>               \n 1 train/test split  5.49     1      5.35 Preprocessor1_Model1\n 2 train/test split  5.64     4      5.78 Preprocessor1_Model1\n 3 train/test split  6.08     6      6.09 Preprocessor1_Model1\n 4 train/test split  5.59     9      5.36 Preprocessor1_Model1\n 5 train/test split  5.49    14      5.60 Preprocessor1_Model1\n 6 train/test split  5.74    15      5.72 Preprocessor1_Model1\n 7 train/test split  5.79    28      5.82 Preprocessor1_Model1\n 8 train/test split  5.73    33      5.84 Preprocessor1_Model1\n 9 train/test split  5.56    54      5.77 Preprocessor1_Model1\n10 train/test split  5.50    72      5.51 Preprocessor1_Model1\n# … with 5,394 more rows\n\n\nThe code below creates a simple plot to examine predicted vs. actual price (log base 10) from the house data.\n\n\n\ncollect_predictions(house_lm_test) %>% \n  ggplot(aes(x = log_price, \n             y = .pred)) +\n  geom_point(alpha = .5, \n             size = .5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, \n              intercept = 0, \n              color = \"darkred\") +\n  labs(x = \"Actual log(price)\", \n       y = \"Predicted log(price)\")\n\n\n\n\nHere is the same plot using the regular price scale(使用 Y 的真实值, 而非转换后的值).\n\n\n\ncollect_predictions(house_lm_test) %>% \n  ggplot(aes(x = 10^log_price, \n             y = 10^.pred)) +\n  geom_point(alpha = .5, \n             size = .5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, \n              intercept = 0, \n              color = \"darkred\") +\n  labs(x = \"Actual price\", \n       y = \"Predicted price\") +\n  scale_x_continuous(labels = scales::dollar_format(scale = .000001, \n                                                    suffix = \"M\")) +\n  scale_y_continuous(labels = scales::dollar_format(scale = .000001, \n                                                    suffix = \"M\"))\n\n\n\n\n9. Question the data and model\n(We’ll go back to step #8 in a moment)\nWhen we use create models, it is important to think about how the model will be used and specifically how the model could do harm. One thing to notice in the graphs above/below is that the price of lower priced homes: 高估 are, on average, overestimated whereas the price of higher priced homes: 低估 are, on average, underestimated.\n\nWhat if this model was used to determine the price of homes for property tax purposes? Then lower priced homes would be overtaxed while higher priced homes would be undertaxed.\nThere are many different ways we might continue to examine this model (eg. are there differences by zipcode) but for now, we’ll move on.\n8. Use the model\nHow might use this model? One simple way is to predict new values. We saw that we could add the predicted values to the test data using the collect_predictions() function. Below, I predict the value for one new observation using the predict() function. We put the values of each variable in a dataset, in this case a tibble(). We need to have values for all the variables that were originally in the dataset passed to the recipe(), 📝 even the evaluation ones that don’t get used in the model. We can have extra variables in there, though, like the one I have called garbage. I show a predicted value (for a linear model, type = \"numeric\") and a confidence interval (type = \"conf_int\"). (简单说就是: 变量名要一样, 只能多不能少)\n📝 NOTE: This is a bit of an aside, but an important one. If I would have used the step_log() function to transform the response variable price in the pre-processing step, rather than transforming it before that, we would see an error message in the predict() below because it would try to run that transformation step, but there wouldn’t be a price variable(这就是要在 recipes 之前进行 Y 的数据转换, 这样的话在运行数据预测的时候才不会出错, 因为新的数据有可能是没有 Y 变量的). In real life, it would usually be the case that you don’t have a value for the variable you are trying to predict.\nI originally tried to solve this problem by adding skip = TRUE to the step_log() function, but then the evaluation metrics in collect_metrics() compared the predicted log price to the actual price - yikes! This is discussed in a few places online - tuning fails when recipe step affects an outcome with skip = TRUE · Issue #47 · tidymodels/workflows’s one.\n\n🔥 📝 : The solution is to transform the response variable before doing any of the modeling steps, as I mentioned in the Data splitting section.\n\n\n\n\npredict(\n  house_lm_fit,\n  new_data = tibble(id = \"0705700390\",\n                    date = ymd(\"2014-09-03\"),\n                    bedrooms = 3,\n                    bathrooms = 2.25,\n                    sqft_living = 2020,\n                    sqft_lot = 8379,\n                    floors = 2,\n                    waterfront = FALSE,\n                    view = 0,\n                    condition = \"3\",\n                    grade = \"7\",\n                    sqft_above = 2020,\n                    sqft_basement = 0,\n                    yr_built = 1994,\n                    yr_renovated = 0,\n                    zipcode = \"98038\",\n                    lat = 47.3828,\n                    long = -122.023,\n                    sqft_living15 = 2020,\n                    sqft_lot15 = 8031,\n                    garbage = \"look, it's garbage\"),\n  type = \"numeric\",\n  level = 0.95\n)\n\n\n# A tibble: 1 x 1\n  .pred\n  <dbl>\n1  5.55\n\npredict(\n  house_lm_fit,\n  new_data = tibble(id = \"0705700390\",\n                    date = ymd(\"2014-09-03\"),\n                    bedrooms = 3,\n                    bathrooms = 2.25,\n                    sqft_living = 2020,\n                    sqft_lot = 8379,\n                    floors = 2,\n                    waterfront = FALSE,\n                    view = 0,\n                    condition = \"3\",\n                    grade = \"7\",\n                    sqft_above = 2020,\n                    sqft_basement = 0,\n                    yr_built = 1994,\n                    yr_renovated = 0,\n                    zipcode = \"98038\",\n                    lat = 47.3828,\n                    long = -122.023,\n                    sqft_living15 = 2020,\n                    sqft_lot15 = 8031,\n                    garbage = \"look, it's garbage\"),\n  type = \"conf_int\",\n  level = 0.95\n)\n\n\n# A tibble: 1 x 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1        5.55        5.56\n\n\nWe could also give it an entire dataset. Here, I just take a sample from the original dataset, add an extra variable (just to show you that you can have more than what you need), and predict with it.\n\n\n\nset.seed(327)\n\nfake_new_data <- house_prices %>% \n  sample_n(20) %>% \n  mutate(extra_var = 1:20)\n\npredict(house_lm_fit, \n        fake_new_data)\n\n\n# A tibble: 20 x 1\n   .pred\n   <dbl>\n 1  5.55\n 2  5.50\n 3  5.61\n 4  5.36\n 5  5.54\n 6  5.59\n 7  5.56\n 8  5.65\n 9  5.34\n10  6.40\n11  5.68\n12  5.80\n13  5.98\n14  5.56\n15  5.49\n16  5.61\n17  5.73\n18  5.40\n19  5.96\n20  5.62\n\n\nSince the predict() function will always return the same number of rows and in the same order as the dataset we put in, we can easily append the prediction to the dataset.\n\n\n\nfake_new_data %>% \n  bind_cols(predict(house_lm_fit,\n                    fake_new_data)) \n\n\n# A tibble: 20 x 23\n   id        date       bedrooms bathrooms sqft_living sqft_lot floors\n   <chr>     <date>        <int>     <dbl>       <int>    <int>  <dbl>\n 1 72023306… 2014-07-21        3      2.5         2020     5613    2  \n 2 66790010… 2014-12-18        3      2.5         1660     7388    2  \n 3 76822003… 2014-07-28        3      2.25        1960     8875    1  \n 4 61502004… 2014-05-13        2      0.75         650     5360    1  \n 5 86455113… 2014-12-01        3      1.75        1810    21138    1  \n 6 93210101… 2015-03-12        3      1.75        1390     8980    1  \n 7 78532708… 2014-08-05        3      2.5         2230     7934    2  \n 8 05100029… 2015-04-07        4      1           1640     4200    1.5\n 9 19016000… 2014-06-26        2      1            720     8040    1  \n10 12250690… 2014-05-05        7      8          13540   307752    3  \n11 11050007… 2015-01-23        3      1.5         1570    10824    2  \n12 36297601… 2014-08-21        3      2.5         2490     4904    2  \n13 75010000… 2014-11-21        4      3.5         3020    12750    2  \n14 86455400… 2014-07-25        3      2           1790     8228    1  \n15 21720002… 2015-03-23        2      1           1150    11250    1  \n16 79670002… 2014-11-21        3      2.5         1930     4000    2  \n17 40778002… 2014-06-10        3      1.5         2010     9480    1  \n18 04250000… 2014-10-21        2      1           1150     5695    1  \n19 95418001… 2014-10-10        5      2.5         3490    18850    1  \n20 43182004… 2014-05-22        3      2.25        1470     1578    2  \n# … with 16 more variables: waterfront <lgl>, view <int>,\n#   condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>, log_price <dbl>, extra_var <int>, .pred <dbl>\n\n\nWe could also add a confidence interval and use the relocate() function to move around some variables.\n\n\n\nfake_new_data %>% \n  bind_cols(predict(house_lm_fit,\n                    fake_new_data)) %>% \n  bind_cols(predict(house_lm_fit,\n                    fake_new_data, \n                    type = \"conf_int\")) %>% \n  # put the pred col after the log_price col\n  relocate(log_price, starts_with(\".pred\"), \n           .after = id)\n\n\n# A tibble: 20 x 25\n   id      log_price .pred .pred_lower .pred_upper date       bedrooms\n   <chr>       <dbl> <dbl>       <dbl>       <dbl> <date>        <int>\n 1 720233…      5.72  5.55        5.54        5.56 2014-07-21        3\n 2 667900…      5.45  5.50        5.49        5.51 2014-12-18        3\n 3 768220…      5.26  5.61        5.60        5.62 2014-07-28        3\n 4 615020…      5.36  5.36        5.35        5.37 2014-05-13        2\n 5 864551…      5.48  5.54        5.53        5.55 2014-12-01        3\n 6 932101…      5.44  5.59        5.58        5.60 2015-03-12        3\n 7 785327…      5.65  5.56        5.55        5.57 2014-08-05        3\n 8 051000…      5.92  5.65        5.64        5.66 2015-04-07        4\n 9 190160…      5.32  5.34        5.33        5.35 2014-06-26        2\n10 122506…      6.36  6.40        6.38        6.43 2014-05-05        7\n11 110500…      5.36  5.68        5.66        5.69 2015-01-23        3\n12 362976…      5.80  5.80        5.79        5.81 2014-08-21        3\n13 750100…      5.93  5.98        5.97        5.99 2014-11-21        4\n14 864554…      5.50  5.56        5.55        5.57 2014-07-25        3\n15 217200…      5.41  5.49        5.48        5.50 2015-03-23        2\n16 796700…      5.54  5.61        5.60        5.62 2014-11-21        3\n17 407780…      5.63  5.73        5.72        5.74 2014-06-10        3\n18 042500…      5.26  5.40        5.38        5.41 2014-10-21        2\n19 954180…      5.96  5.96        5.94        5.97 2014-10-10        5\n20 431820…      5.64  5.62        5.61        5.63 2014-05-22        3\n# … with 18 more variables: bathrooms <dbl>, sqft_living <int>,\n#   sqft_lot <int>, floors <dbl>, waterfront <lgl>, view <int>,\n#   condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>, extra_var <int>\n\n\n❓ What if we don’t want to use the model in this R session?\nThat would be quite a common occurrence. After building a model, we would often want to save the model and use it at a later time to apply to new data. We’ll get into some more complex ways of doing this eventually, but for now, let’s do the following:\nSave the model using saveRDS(). This model is saved to the current project folder (assuming you’re using a project right now), but you could save it anywhere you’d like.\n\n\n\nsaveRDS(house_lm_fit, \"house_lm_fit.rds\")\n\n\n\n\nRead the model back in using readRDS(). Go look at house_lm_read in the environment, and you’ll see that, indeed, it is the workflow we saved! 🤗\n\n\n\nhouse_lm_read <- readRDS(\"house_lm_fit.rds\")\n\n\n\n\nUse the model we read back in to predict new data. Just like we did before, we can use the predict() function to predict new values. I use the same fake_new_data I used before.\n\n\n\nfake_new_data %>% \n  bind_cols(predict(house_lm_read,\n                    fake_new_data)) %>% \n  bind_cols(predict(house_lm_read,\n                    fake_new_data, \n                    type = \"conf_int\")) %>% \n  relocate(log_price, starts_with(\".pred\"), \n           .after = id)\n\n\n# A tibble: 20 x 25\n   id      log_price .pred .pred_lower .pred_upper date       bedrooms\n   <chr>       <dbl> <dbl>       <dbl>       <dbl> <date>        <int>\n 1 720233…      5.72  5.55        5.54        5.56 2014-07-21        3\n 2 667900…      5.45  5.50        5.49        5.51 2014-12-18        3\n 3 768220…      5.26  5.61        5.60        5.62 2014-07-28        3\n 4 615020…      5.36  5.36        5.35        5.37 2014-05-13        2\n 5 864551…      5.48  5.54        5.53        5.55 2014-12-01        3\n 6 932101…      5.44  5.59        5.58        5.60 2015-03-12        3\n 7 785327…      5.65  5.56        5.55        5.57 2014-08-05        3\n 8 051000…      5.92  5.65        5.64        5.66 2015-04-07        4\n 9 190160…      5.32  5.34        5.33        5.35 2014-06-26        2\n10 122506…      6.36  6.40        6.38        6.43 2014-05-05        7\n11 110500…      5.36  5.68        5.66        5.69 2015-01-23        3\n12 362976…      5.80  5.80        5.79        5.81 2014-08-21        3\n13 750100…      5.93  5.98        5.97        5.99 2014-11-21        4\n14 864554…      5.50  5.56        5.55        5.57 2014-07-25        3\n15 217200…      5.41  5.49        5.48        5.50 2015-03-23        2\n16 796700…      5.54  5.61        5.60        5.62 2014-11-21        3\n17 407780…      5.63  5.73        5.72        5.74 2014-06-10        3\n18 042500…      5.26  5.40        5.38        5.41 2014-10-21        2\n19 954180…      5.96  5.96        5.94        5.97 2014-10-10        5\n20 431820…      5.64  5.62        5.61        5.63 2014-05-22        3\n# … with 18 more variables: bathrooms <dbl>, sqft_living <int>,\n#   sqft_lot <int>, floors <dbl>, waterfront <lgl>, view <int>,\n#   condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>, extra_var <int>\n\n\n5. Tuning model parameters\nWith the first model, there weren’t any parameters to tune. Let’s go back to step #5 and look at how the workflow changes when we have to do this extra step.\nNow we are going to try using Least Absolute Shrinkage and Selection Operator (LASSO) regression. This method shrinks some coefficients to 0 based on a penalty term. We will use cross-validation to help us find the best penalty term.\nWe will set up the model similar to how we set up the linear model, but add a set_args() function. The tune() argument to the penalty term is a placeholder. We are telling it that we are going to tune the penalty parameter later.\n\n\n\nhouse_lasso_mod <- \n  # Define a lasso model \n  # I believe default is mixture = 1(LASOO) so probably don't need \n  linear_reg(mixture = 1) %>% \n  # Set the engine to \"glmnet\" \n  set_engine(\"glmnet\") %>% \n  # The parameters we will tune.\n  set_args(penalty = tune()) %>% \n  # Use \"regression\"\n  set_mode(\"regression\")\n\n\n\n\nTo see the arguments available for tuning, go to the Explore Model Arguments section of the parsnip documentation and search the model type and engine you are interested in. Below I printed the arguments we can tune for linear_reg using glmnet (LASSO). We could have also tuned the mixture parameter, but I set it to 1 to explicitly use LASSO.\n\n\nAnd then we create a LASSO workflow. Notice that we’re using the same recipe step that we used in the regular linear model.\n\n\n\nhouse_lasso_wf <- \n  # Set up the workflow\n  workflow() %>% \n  # Add the recipe\n  add_recipe(house_recipe) %>% \n  # Add the modeling\n  add_model(house_lasso_mod)\n\nhouse_lasso_wf\n\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_rm()\n• step_log()\n• step_mutate()\n• step_rm()\n• step_date()\n• step_dummy()\n• step_normalize()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n👀 Here’s where some of the new steps come in. We use the grid_regular() function from the dials library to choose some values of the penalty parameter for us. Alternatively, we could give it a vector of values we want to try.\n\n\n\npenalty_grid <- grid_regular(penalty(),\n                             levels = 20)\npenalty_grid \n\n\n# A tibble: 20 x 1\n    penalty\n      <dbl>\n 1 1   e-10\n 2 3.36e-10\n 3 1.13e- 9\n 4 3.79e- 9\n 5 1.27e- 8\n 6 4.28e- 8\n 7 1.44e- 7\n 8 4.83e- 7\n 9 1.62e- 6\n10 5.46e- 6\n11 1.83e- 5\n12 6.16e- 5\n13 2.07e- 4\n14 6.95e- 4\n15 2.34e- 3\n16 7.85e- 3\n17 2.64e- 2\n18 8.86e- 2\n19 2.98e- 1\n20 1   e+ 0\n\n\nThen, use the tune_grid() function to fit the model using cross-validation for all penalty_grid values and evaluate on all the folds.\n\n\n\nhouse_lasso_tune <- \n  house_lasso_wf %>% \n  tune_grid(\n    resamples = house_cv,\n    grid = penalty_grid\n    )\n\nhouse_lasso_tune\n\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 x 4\n  splits               id    .metrics          .notes          \n  <list>               <chr> <list>            <list>          \n1 <split [12967/3242]> Fold1 <tibble [40 × 5]> <tibble [1 × 1]>\n2 <split [12967/3242]> Fold2 <tibble [40 × 5]> <tibble [1 × 1]>\n3 <split [12967/3242]> Fold3 <tibble [40 × 5]> <tibble [1 × 1]>\n4 <split [12967/3242]> Fold4 <tibble [40 × 5]> <tibble [1 × 1]>\n5 <split [12968/3241]> Fold5 <tibble [40 × 5]> <tibble [1 × 1]>\n\n\nThen look at the cross-validated results in a table.\n\n\n\n# The rmse for each fold:\nhouse_lasso_tune %>% \n  select(id, .metrics) %>% \n  unnest(.metrics) %>% \n  filter(.metric == \"rmse\")\n\n\n# A tibble: 100 x 6\n   id     penalty .metric .estimator .estimate .config              \n   <chr>    <dbl> <chr>   <chr>          <dbl> <chr>                \n 1 Fold1 1   e-10 rmse    standard       0.136 Preprocessor1_Model01\n 2 Fold1 3.36e-10 rmse    standard       0.136 Preprocessor1_Model02\n 3 Fold1 1.13e- 9 rmse    standard       0.136 Preprocessor1_Model03\n 4 Fold1 3.79e- 9 rmse    standard       0.136 Preprocessor1_Model04\n 5 Fold1 1.27e- 8 rmse    standard       0.136 Preprocessor1_Model05\n 6 Fold1 4.28e- 8 rmse    standard       0.136 Preprocessor1_Model06\n 7 Fold1 1.44e- 7 rmse    standard       0.136 Preprocessor1_Model07\n 8 Fold1 4.83e- 7 rmse    standard       0.136 Preprocessor1_Model08\n 9 Fold1 1.62e- 6 rmse    standard       0.136 Preprocessor1_Model09\n10 Fold1 5.46e- 6 rmse    standard       0.136 Preprocessor1_Model10\n# … with 90 more rows\n\n# rmse averaged over all folds:\nhouse_lasso_tune %>% \n  collect_metrics() %>% \n  filter(.metric == \"rmse\") \n\n\n# A tibble: 20 x 7\n    penalty .metric .estimator  mean     n  std_err .config           \n      <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>             \n 1 1   e-10 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n 2 3.36e-10 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n 3 1.13e- 9 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n 4 3.79e- 9 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n 5 1.27e- 8 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n 6 4.28e- 8 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n 7 1.44e- 7 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n 8 4.83e- 7 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n 9 1.62e- 6 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n10 5.46e- 6 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n11 1.83e- 5 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n12 6.16e- 5 rmse    standard   0.135     5 0.00104  Preprocessor1_Mod…\n13 2.07e- 4 rmse    standard   0.135     5 0.00103  Preprocessor1_Mod…\n14 6.95e- 4 rmse    standard   0.135     5 0.000987 Preprocessor1_Mod…\n15 2.34e- 3 rmse    standard   0.136     5 0.000857 Preprocessor1_Mod…\n16 7.85e- 3 rmse    standard   0.142     5 0.000680 Preprocessor1_Mod…\n17 2.64e- 2 rmse    standard   0.161     5 0.000651 Preprocessor1_Mod…\n18 8.86e- 2 rmse    standard   0.190     5 0.00106  Preprocessor1_Mod…\n19 2.98e- 1 rmse    standard   0.228     5 0.00134  Preprocessor1_Mod…\n20 1   e+ 0 rmse    standard   0.228     5 0.00134  Preprocessor1_Mod…\n\n\nAnd, even better, we can visualize the results. We can see that the RMSE stays fairly consistently low until just before \\(10^{-3}\\)\n\n\n\n# Visualize rmse vs. penalty\nhouse_lasso_tune %>% \n  collect_metrics() %>% \n  filter(.metric == \"rmse\") %>% \n  ggplot(aes(x = penalty, y = mean)) +\n  geom_point() +\n  geom_line() +\n  scale_x_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\",scales::math_format(10^.x))) +\n  labs(x = \"penalty\", y = \"rmse\")\n\n\n\n\nWe choose the best penalty parameter as the one with the smallest cross-validated RMSE. The select_best() function does this.\n\n\n\nhouse_lasso_tune %>% \n  show_best(metric = \"rmse\")\n\n\n# A tibble: 5 x 7\n   penalty .metric .estimator  mean     n std_err .config             \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 1   e-10 rmse    standard   0.135     5 0.00104 Preprocessor1_Model…\n2 3.36e-10 rmse    standard   0.135     5 0.00104 Preprocessor1_Model…\n3 1.13e- 9 rmse    standard   0.135     5 0.00104 Preprocessor1_Model…\n4 3.79e- 9 rmse    standard   0.135     5 0.00104 Preprocessor1_Model…\n5 1.27e- 8 rmse    standard   0.135     5 0.00104 Preprocessor1_Model…\n\n\n\n\n\n# Best tuning parameter by smallest rmse\nbest_param <- house_lasso_tune %>% \n  select_best(metric = \"rmse\")\nbest_param\n\n\n# A tibble: 1 x 2\n       penalty .config              \n         <dbl> <chr>                \n1 0.0000000001 Preprocessor1_Model01\n\n\nThere are other ways you can select parameters, like select_by_one_std_err() which “selects the most simple model that is within one standard error of the numerically optimal results”. To use this, we need at least one more argument: the parameter to sort the model from most simple to most complex. So, if using glmnet’s penalty parameter, since a bigger penalty will be a simpler model, I should put desc(penalty) in as the argument.\n\n\n\n# Best tuning parameter by smallest rmse\none_se_param <- house_lasso_tune %>% \n  select_by_one_std_err(metric = \"rmse\", desc(penalty))\none_se_param\n\n\n# A tibble: 1 x 9\n   penalty .metric .estimator  mean     n std_err .config .best .bound\n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>   <dbl>  <dbl>\n1 0.000695 rmse    standard   0.135     5 9.87e-4 Prepro… 0.135  0.136\n\n\nBecause a larger penalty parameter will fit a simpler model (more terms will likely go to zero). I’ll go with the one_se_param, especially since the RMSE is so close to the “best” model’s RMSE.\nOnce we choose the parameter we want, we adjust the workflow to include the best tuning parameter using the finalize_workflow() function.\n\n\n\nhouse_lasso_final_wf <- house_lasso_wf %>% \n  finalize_workflow(one_se_param)\nhouse_lasso_final_wf\n\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_rm()\n• step_log()\n• step_mutate()\n• step_rm()\n• step_date()\n• step_dummy()\n• step_normalize()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.00069519279617756\n  mixture = 1\n\nComputational engine: glmnet \n\n\nNow we could fit this to the training data and look at the resulting model. We can see a few of the terms have coefficients of 0 (although not as many as I would have expected).\n\n\n\nhouse_lasso_final_mod <- house_lasso_final_wf %>% \n  fit(data = house_training)\n\nhouse_lasso_final_mod %>% \n  pull_workflow_fit() %>% \n  tidy() \n\n\n# A tibble: 31 x 3\n   term        estimate  penalty\n   <chr>          <dbl>    <dbl>\n 1 (Intercept)   5.67   0.000695\n 2 bedrooms     -0.0135 0.000695\n 3 bathrooms     0.0249 0.000695\n 4 sqft_living   0.0587 0.000695\n 5 sqft_lot     -0.0138 0.000695\n 6 floors        0.0127 0.000695\n 7 waterfront    0.0161 0.000695\n 8 view         -0.0173 0.000695\n 9 sqft_above    0.0213 0.000695\n10 basement     -0.0180 0.000695\n# … with 21 more rows\n\n\nWe can also visualize variable importance.\n\n\n\n# Visualize variable importance\nhouse_lasso_final_mod %>% \n  pull_workflow_fit() %>% \n  vip()\n\n\n\n\nLastly, we apply the model to the test data and examine some final metrics. We also show the metrics from the regular linear model. It looks like performance for the LASSO model is ever so slightly better, but just barely. It’s also a good sign that these RMSE’s are similar to the cross-validated RMSE’s.\n\n\n\n# Fit model with best tuning parameter(s) to training data and apply to test data\nhouse_lasso_test <- house_lasso_final_wf %>% \n  last_fit(house_split)\n\n# Metrics for model applied to test data\nhouse_lasso_test %>% \n  collect_metrics()\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.136 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n\n# Compare to regular linear regression results\ncollect_metrics(house_lm_test)\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.136 Preprocessor1_Model1\n2 rsq     standard       0.654 Preprocessor1_Model1\n\n\nRef\nAdvanced Data Science in R: Machine Learning review with an intro to the tidymodels package\n\n\n\n",
    "preview": "posts/2021-06-24-ml-review/ml-review_images/image-20210624170933171.png",
    "last_modified": "2021-06-24T17:18:01+08:00",
    "input_file": {},
    "preview_width": 1244,
    "preview_height": 1014
  },
  {
    "path": "posts/2021-02-03-current-state-of-experimental-design-r-packages/",
    "title": "Current state of R packages for the design of experiments",
    "description": "Your analytical toolkit matters very little if the data are no good. Ideally you want to know to how the data were collected before delving into the analysis of the data; better yet, get involved _before_ the collection of data and design its collection. In this post I explore some of the top downloaded R packages for the design of experiments and analysis of experimental data.",
    "author": [
      {
        "name": "Jixing Liu",
        "url": "https://emitanaka.org"
      }
    ],
    "date": "2021-02-03",
    "categories": [
      "experimental design",
      "R"
    ],
    "contents": "\n\nContents\nData collection\nExperimental data\n\nDesign and analysis of experiments\nBigram of DoE package titles and descriptions\nNetwork of DoE package imports and dependencies\nCRAN download logs\nTop 5 DoE packages\n\nR-packages\nAlgDesign\nagricolae\nlhs\nez\nDoE.base\n\n\n\n\n\n.toggle-code {\n  display: none;\n}\n\nbutton {\n  border-radius: 10px;\n  color: var(--aside-color, rgba(0, 0, 0, 0.6));\n  border-color: var(--aside-color, rgba(0, 0, 0, 0.6));\n}\n\n.scroll-output {\n  height: 200px;\n  overflow-y: scroll!important;\n}\n\n\n\n\nClick Me\n\nto see all code in this article. You can also find the link to the source Rmd file at the footer.\n\n\n\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(lubridate)\nlibrary(cranlogs)\nlibrary(glue)\nlibrary(scales)\nlibrary(colorspace)\nlibrary(tidytext)\nlibrary(pluralize)\nlibrary(kableExtra)\nlibrary(igraph)\nlibrary(ggraph)\n\nmyggtheme <- \n  theme(panel.background = element_rect(fill = NA),\n        panel.grid = element_line(color = \"#f6e5ee\"),\n        axis.text = element_text(color = \"#6A3FAD\"),\n        axis.line = element_line(color = \"#6A3FAD\", size = 0.7),\n        axis.ticks.length = unit(1.4, \"mm\"),\n        axis.ticks = element_line(color = \"#6A3FAD\", size = 0.7),\n        axis.title = element_text(color = \"#6A3FAD\", face = \"bold\"),\n        strip.background = element_rect(color = \"#6A3FAD\",\n                                        fill = \"#6A3FAD\"),\n        strip.text = element_text(color = \"white\"),\n        plot.title.position = \"plot\",\n        plot.title = element_text(color = \"#6A3FAD\", face = \"bold\")) \n\n\n\n\n\n\n\n# Thanks to Dirk Eddelbuettel's answer on SO:\n# https://stackoverflow.com/questions/11560865/list-and-description-of-all-packages-in-cran-from-within-r\nurl <- paste0(getOption(\"repos\")[\"CRAN\"], \"web/packages/packages.rds\")\ndb <- readRDS(url(url)) %>% \n  as.data.frame()\n\n\n\n\n\n\n\nnanalysis <- db %>% \n  filter(str_detect(tolower(Title), \"analysis\")) %>% \n  nrow()\n\nndesign <- db %>% \n  filter(str_detect(tolower(Title), \"design\")) %>% \n  nrow()\n\n\n\n\nData collection\nAs many know, it doesn’t matter how good your analytical tools is if your data are rubbish. This sentiment is often captured in the expression “garbage in, garbage out”. It’s something we all seem to know but there is still a tendency for many of us to place a greater focus on the analysis1. This is perhaps all natural given that a potential for discovery is just so much more exciting than ensuring the quality of the collected data.\nSo what is considered as good quality data? A lack of error in the data? Data containing enough range of variables and sample size for the downstream analysis? Giving an explicit definition of a good quality data is a fraught exercise, but if you know how the data were collected then you can better perform the initial data analysis (Chatfield 1985) to weed out (or fix) potential poor quality data. This step will likely get more value out of the data than fitting complex models to poor quality data.\nBetter than knowing how the data were collected, if you can design the collection of data so that it’s optimised for the purpose of the analysis2, then you can potentially get even a better value out of your data. Not all data collection starts with an explicit analytical plan though. Furthermore, you may have very little control of how the data are collected. Often these are observational data or making a secondary use of experimental data. This article will focus on data collection of an experiment where you have some control of the collection process.\nExperimental data\nAll experiments are conducted with some objective in mind. This could be that a scientist may wish to test their hypothesis, a manufacturer wants to know which manufacturing process is better or a researcher wants to understand some cause-and-effect relationships. A characteristic part of an experiment is that the experimenter has control over some explanatory variables. In a comparative experiment, the control is over the allocation of treatments to subjects. Designing an experiment in the statistics discipline usually focus on this allocation, although it’s important to keep in mind that there are other decision factors in an experiment.\nData that are collected from experiments are what we refer to as experimental data. Because it was collected with some objective in mind followed by some data collection plan, experimental data are often thought of to be better quality than observational data. But then again if you can’t quantify the quality of data, you can’t really tell. Certain scientific claims (e.g. causation, better treatment) can only be substantiated by experiments and so experimental data is held to a higher standard in general.\nDesign and analysis of experiments\n\n\n\ndat_DoE <- read_html(\"https://cran.r-project.org/web/views/ExperimentalDesign.html\")\ndate_download <- Sys.Date()\ncran_names <- available.packages() %>% \n  rownames() %>% \n  unique() # it should be unique\ndoe_pkgs <- dat_DoE %>% \n  html_nodes(\"li\") %>% \n  html_nodes(\"a\") %>% \n  html_text() %>% \n  .[. %in% cran_names] %>% \n  unique()\n\ndat_survey <- read_html(\"https://cran.r-project.org/web/views/OfficialStatistics.html\")\nsurvey_pkgs <- dat_survey %>% \n  html_nodes(\"li\") %>% \n  html_nodes(\"a\") %>% \n  html_text() %>% \n  .[. %in% cran_names] %>% \n  unique()\n\n\n\n\nThere are all together 113 R-packages in the CRAN Task View of Design of Experiments & Analysis of Experimental Data as of 2021-02-07.3 I’m going to refer these packages as DoE packages, although there are some packages in the mix that are more about the analysis of experimental data rather than the design of experiments and there are some packages that are missing in the list (e.g. DeclareDesign). The DoE packages make up about 0.7% of the 17,036 packages available in CRAN.\nThe DoE packages don’t include survey design. These instead belong to the CRAN Task View of Official Statistics & Survey Methodology which contains 132 packages. While some surveys are part of an experimental study, most often they generate observational data.\nBelow I have a number of different analysis for these DoE packages. If you push the button on the top right corner of this article, you can toggle the display for the code or alternatively you can have a look at the source Rmd document.\nBigram of DoE package titles and descriptions\n\n\n\nstop_words_ext <- c(stop_words$word, \"doi\")\n\ndoe_db <- db %>% \n  filter(Package %in% doe_pkgs) %>% \n  mutate(Description = str_replace_all(Description, \"\\n\", \" \"),\n         Description = str_squish(Description),\n         Title = str_replace_all(Title, \"\\n\", \" \"))\n\nbigram_tab <- function(data, col) {\n  data %>% \n    unnest_tokens(word, {{col}}, token = \"ngrams\", n = 2) %>% \n    separate(word, c(\"word1\", \"word2\"), sep = \" \") %>% \n    mutate(word1 = singularize(word1),\n           word2 = singularize(word2)) %>% \n    # don't count the same bigram within the same package\n    distinct(Package, word1, word2) %>% \n    filter(!word1 %in% stop_words_ext,\n           !word2 %in% stop_words_ext,\n           !str_detect(word1, \"^[0-9.]+$\"),\n           !str_detect(word2, \"^[0-9.]+$\")) %>% \n    count(word1, word2, sort = TRUE)  \n}\n\n\n\n\n\n\n\nbigram_tab(doe_db, Description) %>% \n  filter(n > 4) %>% \n  mutate(word = paste(word1, word2)) %>% \n  select(word, n) %>% \n  kbl(caption = \"The bigram of the R-package _descriptions_ as provided in the DESCRIPTION file in CRAN.\", \n               col.names = c(\"Bigram\", \"Count\")) %>% \n  kable_classic(full_width = FALSE)\n\n\n\n\n\n\n\nbigram_tab(doe_db, Title) %>% \n  filter(n > 3) %>% \n  mutate(word = paste(word1, word2)) %>% \n  select(word, n) %>% \n  kbl(caption = \"The bigram of the R-package _titles_ as provided in the DESCRIPTION file in CRAN.\", \n               col.names = c(\"Bigram\", \"Count\")) %>% \n  kable_classic(full_width = FALSE)\n\n\n\n\nTable 1 shows the most common bigrams in the title of the DoE packages. It’s perhaps not surprising but the words “optimal design” and “experimental design” are the top. It’s also likely that the words “design of experiments” appears often but because this is a bigram (two consecutive words) so it doesn’t appear. You might then wonder if that’s the case words like “design of” or “of experiments” should make an appearance, however “of” is a stop word and these are filtered out otherwise unwanted bigrams come up on the top.\nThere are couple of words like “clinical trial” and “dose finding” that suggests applications in medical experiments, as well as “microarray experiment” that suggests application in bioinformatics.\n\n\nTable 1: The bigram of the R-package titles as provided in the DESCRIPTION file in CRAN.\n\n\nBigram\n\n\nCount\n\n\noptimal design\n\n\n10\n\n\nexperimental design\n\n\n8\n\n\nclinical trial\n\n\n5\n\n\ndose finding\n\n\n5\n\n\nsequential design\n\n\n5\n\n\nblock design\n\n\n4\n\n\nmicroarray experiment\n\n\n4\n\n\nresponse surface\n\n\n4\n\n\nThe title alone might be too succinct for text analysis so I also had a look at the most common bigrams in the description of the DoE packages as shown in Table 2. The counts in Table 2 (and also Table 1) is across the DoE packages. To be more clear, even if the bigram is mentioned multiple times within the description, it’s only counted once per package. This removes the inflation of the counts due to one package mentioning the same bigram over and over again.\nAgain not surprisingly “experimental design” and “optimal design” comes on top in the DoE package descriptions. The words “graphical user” and “user interface” implies that the trigram “graphical user interface” was probably common.\n\n\nTable 2: The bigram of the R-package descriptions as provided in the DESCRIPTION file in CRAN.\n\n\nBigram\n\n\nCount\n\n\nexperimental design\n\n\n11\n\n\noptimal design\n\n\n10\n\n\npackage provide\n\n\n7\n\n\nresponse surface\n\n\n7\n\n\nfactorial design\n\n\n6\n\n\ngraphical user\n\n\n6\n\n\nuser interface\n\n\n6\n\n\nblock design\n\n\n5\n\n\ncontour plot\n\n\n5\n\n\ndesign based\n\n\n5\n\n\neffect model\n\n\n5\n\n\nfractional factorial\n\n\n5\n\n\nmicroarray experiment\n\n\n5\n\n\nmixed effect\n\n\n5\n\n\nprovide function\n\n\n5\n\n\nsample size\n\n\n5\n\n\nsequential design\n\n\n5\n\n\nNetwork of DoE package imports and dependencies\n\n\n\ndoe_imports <- doe_db %>% \n  mutate(Depends = str_replace_all(Depends, \"\\n\", \" \"),\n         Depends = str_replace_all(Depends, fixed(\"(\"), \" (\"),\n         Imports = str_replace_all(Imports, \"\\n\", \" \"),\n         Imports = str_replace_all(Imports, fixed(\"(\"), \" (\"),\n         imports = str_c(Depends, Imports, sep = \",\"),\n         imports = str_split(imports, \",\"),\n         imports = map(imports, ~{\n                    str_squish(.x) %>% \n                      word() %>% \n                      .[.!=\"\"]}\n           ),\n         imports_doe = map(imports, ~.x[.x %in% doe_pkgs])) %>% \n  select(Package, imports_doe) %>% \n  unnest_longer(imports_doe) %>% \n  filter(!is.na(imports_doe)) %>% \n  rename(from = imports_doe, to = Package) %>% \n  select(from, to)\n\n\n\n\nFigure 1 shows the imports and dependency between the DoE packages. We can see here that DoE.wrapper imports a fair number of DoE packages that results in the major network cluster see in Figure 1. AlgDesign and DoE.base are imported into four other DoE packages so form an important base in the DoE world.\n\n\n\ngraph_from_data_frame(doe_imports) %>% \n  ggraph(layout = 'fr') +\n  geom_edge_link(aes(start_cap = label_rect(node1.name),\n                     end_cap = label_rect(node2.name)), \n                 arrow = arrow(length = unit(2, 'mm')),\n                 color = \"#6A3FAD\") + \n  geom_node_text(aes(label = name),\n                 color = \"#6A3FAD\") +\n  theme(panel.background = element_rect(fill = \"#f6e5ee\",\n                                        color = \"#6A3FAD\"),\n        plot.margin = margin(20, 20, 20, 20))\n\n\n\n\n\n\n\n\nFigure 1: The network of imports and dependency among DoE packages alone. Each node represents a DoE package. DoE packages with no imports or dependency on other DoE packages are excluded. Each arrow represents the relationship between the packages such that the package on the tail is used by package on the head of the arrow.\n\n\n\nCRAN download logs\n\n\n\nend <- Sys.Date() - 2 # usually 1-2 days are not available yet\nstart <- end - years(5) + days(2)\ndldat <- cran_downloads(doe_pkgs, from = start, to = end)\n\n\n\n\n\n\n\ndldat %>% \n    group_by(package) %>% \n    summarise(total = sum(count)) %>%\n  ggplot(aes(total)) + \n  geom_histogram(color = \"white\", fill = \"#6A3FAD\") + \n  scale_x_log10(label = comma) + \n  myggtheme + \n  labs(x = glue(\"Total download counts from {start} to {end}\"),\n       y = \"Number of packages\") +\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\nFigure 2 shows the distribution of the total download counts over the last 5 years4 of the DoE packages. This graph doesn’t take into account that some DoE packages may only have been on CRAN in the last 5 years so the counts are in favour of DoE packages that’s been on CRAN longer.\n\n\n\n\nFigure 2: Histogram of the total download count over last 5 years of the DoE packages.\n\n\n\nTop 5 DoE packages\n\n\n\nntop <- 5\n\ntop5sum_df <- dldat %>% \n  group_by(package) %>% \n  summarise(total = sum(count)) %>% \n  ungroup() %>% \n  slice_max(order_by = total, n = ntop)\n\ntop5 <- top5sum_df %>% \n  pull(package) \n\ntop5_df <- dldat %>% \n  filter(package %in% top5) %>% \n  mutate(package = fct_reorder(package, count, function(x) -sum(x))) \n\n\n\n\nThe top 5 downloaded DoE packages at the time of this writing are AlgDesign, agricolae, lhs, ez, and DoE.base. You can see the download counts in Figure 3.\n\n\n\ntop5sum_df %>% \n  mutate(package = fct_reorder(package, total)) %>% \n  ggplot(aes(total, package)) +\n  geom_col(aes(fill = package)) +\n  labs(x = glue(\"Total downloads from {start} to {end}\"),\n       y = \"Package\") + \n  scale_x_continuous(labels = comma, expand = c(0, 0)) +\n  myggtheme + \n  scale_fill_discrete_qualitative(rev = TRUE) + \n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nFigure 3: The above barplot shows the total downloads of the top 5 downloaded DoE packages from the period 2016-02-07 to 2021-02-05.\n\n\n\nWe can have a look at further examination of the top 5 DoE packages by looking at the daily download counts as shown in Figure 3. The download counts are the raw values and these include downloads by CRAN mirror and bots. There is a noticeable spike when there is an update to the CRAN package. This is partly because when there is a new version of the package, when you install other packages that depend or import it then R will prompt you to install the new version. This means that the download counts are inflated and to some extent you can artificially boost them by making regular CRAN updates. The adjustedcranlogs (Morgan-Wall 2017) makes a nice attempt to adjust the raw counts based on a certain heuristic. I didn’t use it since the adjustment is stochastic and I appear to have hit a bug.\n\n\n\npkg_url <- \"https://cran.r-project.org/web/packages/{pkg}/index.html\"\npkg_archive <- \"https://cran.r-project.org/src/contrib/Archive/{pkg}/\"\npkg_updates <- map(top5, function(pkg) {\n    last_update <- read_html(glue(pkg_url)) %>% \n      html_table() %>% \n      .[[1]] %>% \n      filter(X1==\"Published:\") %>% \n      pull(X2) %>% \n      ymd()\n      \n    archive_dates <- tryCatch({ \n        read_html(glue(pkg_archive)) %>% \n          html_table() %>%\n          .[[1]] %>% \n          pull(`Last modified`) %>% \n          ymd_hm() %>% \n          na.omit() %>% \n          as.Date()\n      }, error = function(e) {\n        NULL\n      })\n    c(archive_dates, last_update)\n  })\nnames(pkg_updates) <- top5\n\nupdates <- unlist(pkg_updates) %>% \n  enframe(\"package\", \"update\") %>% \n  # unlist converts date to integers\n  mutate(update = as.Date(update, origin = \"1970-01-01\"),\n         # need to get rid of the numbers appended to pkg names\n         package = str_extract(package, paste0(top5, collapse=\"|\")),\n         package = factor(package, levels = top5)) %>% \n  filter(between(update, start, end))\n\n\n\n\n\n\n\nggplot(top5_df, aes(date, count, color = package)) +\n  # add shadow lines\n  geom_line(data = rename(top5_df, package2 = package), \n            color = \"gray\", aes(group = package2)) +\n  # add date when package was updated\n  geom_vline(data = updates, aes(xintercept = update),\n             linetype = \"dashed\", color = \"#6A3FAD\") + \n  # the trend line\n  geom_line() +\n  scale_y_log10() +\n  facet_grid(package ~ .) + \n  labs(title = glue(\"Top 5 downloaded DoE packages from {start} to {end}\")) + \n  scale_color_discrete_qualitative() +\n  guides(color = FALSE) +\n  myggtheme\n\n\n\n\n\n\n\n\nFigure 4: The above plot shows the daily downloads of the top 5 downloaded DoE packages from the period 2016-02-07 to 2021-02-05. The vertical dotted bar corresponds to the date that a new version of the corresponding package was released on CRAN.\n\n\n\nR-packages\nHere we have a closer look at the functions of the top 5 downloaded DoE packages below ordered by their download counts.\nAlgDesign  CRAN  GitHub  Wheeler (2019)Algorithmic Experimental Design\nOriginally written by  Bob Wheeler but  Jerome Braun have taken over maintenance of the package.\nagricolae  CRAN  de Mendiburu (2020)Statistical Procedures for Agricultural Research\nWritten and maintained by  Felipe de Mendiburu\nlhs  CRAN  GitHub  Carnell (2020)Latin Hypercube Samples\nWritten and maintained by  Rob Carnell\nez  CRAN  GitHub  Lawrence (2016)Easy Analysis and Visualization of Factorial Experiments\nWritten and maintained by  Michael A. Lawrence\nDoE.base  CRAN  Grömping (2018)Full Factorials, Orthogonal Arrays and Base Utilities for DoE Packages\nWritten and maintained by  Ulrike Groemping.\n\nInterestingly these top 5 DoE packages have only one active author. Bob Wheeler doesn’t seem to actively contribute to AlgDesign any longer; and there are two contributors for DoE.base but are not listed as authors.\nBefore we look at the packages, let’s set a seed so we can reproduce the results.\n\n\nset.seed(2021)\n\n\n\nAlgDesign\nTo start off, we begin with the most downloaded DoE package, AlgDesign. The examples below are taken directly from the vignette of the AlgDesign package.\n\n\nlibrary(AlgDesign)\n\n\n\nYou can create a balanced incomplete block design using the optBlock function. It’s using an optimal design framework where the default criterion is D criterion and the implied model is given in the first argument.\n\n\nBIB <- optBlock(~ ., \n                withinData = factor(1:7), \n                blocksize = rep(3, 7))\nBIB\n\n\n$D\n[1] 0.08033556\n\n$diagonality\n[1] 0.692\n\n$Blocks\n$Blocks$B1\n  X1\n1  1\n3  3\n4  4\n\n$Blocks$B2\n  X1\n2  2\n4  4\n5  5\n\n$Blocks$B3\n  X1\n4  4\n6  6\n7  7\n\n$Blocks$B4\n  X1\n3  3\n5  5\n6  6\n\n$Blocks$B5\n  X1\n2  2\n3  3\n7  7\n\n$Blocks$B6\n  X1\n1  1\n2  2\n6  6\n\n$Blocks$B7\n  X1\n1  1\n5  5\n7  7\n\n\n$design\n   X1\n1   1\n3   3\n4   4\n2   2\n41  4\n5   5\n42  4\n6   6\n7   7\n31  3\n51  5\n61  6\n21  2\n32  3\n71  7\n11  1\n22  2\n62  6\n12  1\n52  5\n72  7\n\n$rows\n [1] 1 3 4 2 4 5 4 6 7 3 5 6 2 3 7 1 2 6 1 5 7\n\nAlgDesign also includes helper functions to generate a factorial structure.\n\n\ndat <- gen.factorial(2, 7)\ndat\n\n\n    X1 X2 X3 X4 X5 X6 X7\n1   -1 -1 -1 -1 -1 -1 -1\n2    1 -1 -1 -1 -1 -1 -1\n3   -1  1 -1 -1 -1 -1 -1\n4    1  1 -1 -1 -1 -1 -1\n5   -1 -1  1 -1 -1 -1 -1\n6    1 -1  1 -1 -1 -1 -1\n7   -1  1  1 -1 -1 -1 -1\n8    1  1  1 -1 -1 -1 -1\n9   -1 -1 -1  1 -1 -1 -1\n10   1 -1 -1  1 -1 -1 -1\n11  -1  1 -1  1 -1 -1 -1\n12   1  1 -1  1 -1 -1 -1\n13  -1 -1  1  1 -1 -1 -1\n14   1 -1  1  1 -1 -1 -1\n15  -1  1  1  1 -1 -1 -1\n16   1  1  1  1 -1 -1 -1\n17  -1 -1 -1 -1  1 -1 -1\n18   1 -1 -1 -1  1 -1 -1\n19  -1  1 -1 -1  1 -1 -1\n20   1  1 -1 -1  1 -1 -1\n21  -1 -1  1 -1  1 -1 -1\n22   1 -1  1 -1  1 -1 -1\n23  -1  1  1 -1  1 -1 -1\n24   1  1  1 -1  1 -1 -1\n25  -1 -1 -1  1  1 -1 -1\n26   1 -1 -1  1  1 -1 -1\n27  -1  1 -1  1  1 -1 -1\n28   1  1 -1  1  1 -1 -1\n29  -1 -1  1  1  1 -1 -1\n30   1 -1  1  1  1 -1 -1\n31  -1  1  1  1  1 -1 -1\n32   1  1  1  1  1 -1 -1\n33  -1 -1 -1 -1 -1  1 -1\n34   1 -1 -1 -1 -1  1 -1\n35  -1  1 -1 -1 -1  1 -1\n36   1  1 -1 -1 -1  1 -1\n37  -1 -1  1 -1 -1  1 -1\n38   1 -1  1 -1 -1  1 -1\n39  -1  1  1 -1 -1  1 -1\n40   1  1  1 -1 -1  1 -1\n41  -1 -1 -1  1 -1  1 -1\n42   1 -1 -1  1 -1  1 -1\n43  -1  1 -1  1 -1  1 -1\n44   1  1 -1  1 -1  1 -1\n45  -1 -1  1  1 -1  1 -1\n46   1 -1  1  1 -1  1 -1\n47  -1  1  1  1 -1  1 -1\n48   1  1  1  1 -1  1 -1\n49  -1 -1 -1 -1  1  1 -1\n50   1 -1 -1 -1  1  1 -1\n51  -1  1 -1 -1  1  1 -1\n52   1  1 -1 -1  1  1 -1\n53  -1 -1  1 -1  1  1 -1\n54   1 -1  1 -1  1  1 -1\n55  -1  1  1 -1  1  1 -1\n56   1  1  1 -1  1  1 -1\n57  -1 -1 -1  1  1  1 -1\n58   1 -1 -1  1  1  1 -1\n59  -1  1 -1  1  1  1 -1\n60   1  1 -1  1  1  1 -1\n61  -1 -1  1  1  1  1 -1\n62   1 -1  1  1  1  1 -1\n63  -1  1  1  1  1  1 -1\n64   1  1  1  1  1  1 -1\n65  -1 -1 -1 -1 -1 -1  1\n66   1 -1 -1 -1 -1 -1  1\n67  -1  1 -1 -1 -1 -1  1\n68   1  1 -1 -1 -1 -1  1\n69  -1 -1  1 -1 -1 -1  1\n70   1 -1  1 -1 -1 -1  1\n71  -1  1  1 -1 -1 -1  1\n72   1  1  1 -1 -1 -1  1\n73  -1 -1 -1  1 -1 -1  1\n74   1 -1 -1  1 -1 -1  1\n75  -1  1 -1  1 -1 -1  1\n76   1  1 -1  1 -1 -1  1\n77  -1 -1  1  1 -1 -1  1\n78   1 -1  1  1 -1 -1  1\n79  -1  1  1  1 -1 -1  1\n80   1  1  1  1 -1 -1  1\n81  -1 -1 -1 -1  1 -1  1\n82   1 -1 -1 -1  1 -1  1\n83  -1  1 -1 -1  1 -1  1\n84   1  1 -1 -1  1 -1  1\n85  -1 -1  1 -1  1 -1  1\n86   1 -1  1 -1  1 -1  1\n87  -1  1  1 -1  1 -1  1\n88   1  1  1 -1  1 -1  1\n89  -1 -1 -1  1  1 -1  1\n90   1 -1 -1  1  1 -1  1\n91  -1  1 -1  1  1 -1  1\n92   1  1 -1  1  1 -1  1\n93  -1 -1  1  1  1 -1  1\n94   1 -1  1  1  1 -1  1\n95  -1  1  1  1  1 -1  1\n96   1  1  1  1  1 -1  1\n97  -1 -1 -1 -1 -1  1  1\n98   1 -1 -1 -1 -1  1  1\n99  -1  1 -1 -1 -1  1  1\n100  1  1 -1 -1 -1  1  1\n101 -1 -1  1 -1 -1  1  1\n102  1 -1  1 -1 -1  1  1\n103 -1  1  1 -1 -1  1  1\n104  1  1  1 -1 -1  1  1\n105 -1 -1 -1  1 -1  1  1\n106  1 -1 -1  1 -1  1  1\n107 -1  1 -1  1 -1  1  1\n108  1  1 -1  1 -1  1  1\n109 -1 -1  1  1 -1  1  1\n110  1 -1  1  1 -1  1  1\n111 -1  1  1  1 -1  1  1\n112  1  1  1  1 -1  1  1\n113 -1 -1 -1 -1  1  1  1\n114  1 -1 -1 -1  1  1  1\n115 -1  1 -1 -1  1  1  1\n116  1  1 -1 -1  1  1  1\n117 -1 -1  1 -1  1  1  1\n118  1 -1  1 -1  1  1  1\n119 -1  1  1 -1  1  1  1\n120  1  1  1 -1  1  1  1\n121 -1 -1 -1  1  1  1  1\n122  1 -1 -1  1  1  1  1\n123 -1  1 -1  1  1  1  1\n124  1  1 -1  1  1  1  1\n125 -1 -1  1  1  1  1  1\n126  1 -1  1  1  1  1  1\n127 -1  1  1  1  1  1  1\n128  1  1  1  1  1  1  1\n\nThis can be an input to specify the design using another function, say with optFederov which uses Federov’s exchange algorithm to generate the design.\n\n\ndesF <- optFederov(~ .^2, \n                   data = dat,\n                   nTrials = 32,\n                   nRepeats = 100)\ndesF\n\n\n$D\n[1] 0.8867999\n\n$A\n[1] 1.296784\n\n$Ge\n[1] 0.412\n\n$Dea\n[1] 0.241\n\n$design\n    X1 X2 X3 X4 X5 X6 X7\n4    1  1 -1 -1 -1 -1 -1\n5   -1 -1  1 -1 -1 -1 -1\n10   1 -1 -1  1 -1 -1 -1\n11  -1  1 -1  1 -1 -1 -1\n16   1  1  1  1 -1 -1 -1\n17  -1 -1 -1 -1  1 -1 -1\n23  -1  1  1 -1  1 -1 -1\n28   1  1 -1  1  1 -1 -1\n30   1 -1  1  1  1 -1 -1\n33  -1 -1 -1 -1 -1  1 -1\n38   1 -1  1 -1 -1  1 -1\n44   1  1 -1  1 -1  1 -1\n50   1 -1 -1 -1  1  1 -1\n51  -1  1 -1 -1  1  1 -1\n56   1  1  1 -1  1  1 -1\n61  -1 -1  1  1  1  1 -1\n66   1 -1 -1 -1 -1 -1  1\n67  -1  1 -1 -1 -1 -1  1\n72   1  1  1 -1 -1 -1  1\n76   1  1 -1  1 -1 -1  1\n77  -1 -1  1  1 -1 -1  1\n84   1  1 -1 -1  1 -1  1\n86   1 -1  1 -1  1 -1  1\n90   1 -1 -1  1  1 -1  1\n95  -1  1  1  1  1 -1  1\n100  1  1 -1 -1 -1  1  1\n105 -1 -1 -1  1 -1  1  1\n110  1 -1  1  1 -1  1  1\n111 -1  1  1  1 -1  1  1\n117 -1 -1  1 -1  1  1  1\n123 -1  1 -1  1  1  1  1\n128  1  1  1  1  1  1  1\n\n$rows\n [1]   4   5  10  11  16  17  23  28  30  33  38  44  50  51  56  61\n[17]  66  67  72  76  77  84  86  90  95 100 105 110 111 117 123 128\n\nIf you want to further randomise within blocks, you can pass the above result to optBlock.\n\n\ndesFBlk <- optBlock(~ .^2, \n                    withinData = desF$design,\n                    blocksizes = rep(8, 4),\n                    nRepeats = 20)\n\ndesFBlk\n\n\n$D\n[1] 0.8049815\n\n$diagonality\n[1] 0.836\n\n$Blocks\n$Blocks$B1\n    X1 X2 X3 X4 X5 X6 X7\n4    1  1 -1 -1 -1 -1 -1\n17  -1 -1 -1 -1  1 -1 -1\n23  -1  1  1 -1  1 -1 -1\n33  -1 -1 -1 -1 -1  1 -1\n77  -1 -1  1  1 -1 -1  1\n84   1  1 -1 -1  1 -1  1\n90   1 -1 -1  1  1 -1  1\n123 -1  1 -1  1  1  1  1\n\n$Blocks$B2\n    X1 X2 X3 X4 X5 X6 X7\n10   1 -1 -1  1 -1 -1 -1\n16   1  1  1  1 -1 -1 -1\n28   1  1 -1  1  1 -1 -1\n50   1 -1 -1 -1  1  1 -1\n61  -1 -1  1  1  1  1 -1\n67  -1  1 -1 -1 -1 -1  1\n86   1 -1  1 -1  1 -1  1\n100  1  1 -1 -1 -1  1  1\n\n$Blocks$B3\n    X1 X2 X3 X4 X5 X6 X7\n5   -1 -1  1 -1 -1 -1 -1\n11  -1  1 -1  1 -1 -1 -1\n30   1 -1  1  1  1 -1 -1\n44   1  1 -1  1 -1  1 -1\n56   1  1  1 -1  1  1 -1\n66   1 -1 -1 -1 -1 -1  1\n95  -1  1  1  1  1 -1  1\n110  1 -1  1  1 -1  1  1\n\n$Blocks$B4\n    X1 X2 X3 X4 X5 X6 X7\n38   1 -1  1 -1 -1  1 -1\n51  -1  1 -1 -1  1  1 -1\n72   1  1  1 -1 -1 -1  1\n76   1  1 -1  1 -1 -1  1\n105 -1 -1 -1  1 -1  1  1\n111 -1  1  1  1 -1  1  1\n117 -1 -1  1 -1  1  1  1\n128  1  1  1  1  1  1  1\n\n\n$design\n    X1 X2 X3 X4 X5 X6 X7\n4    1  1 -1 -1 -1 -1 -1\n17  -1 -1 -1 -1  1 -1 -1\n23  -1  1  1 -1  1 -1 -1\n33  -1 -1 -1 -1 -1  1 -1\n77  -1 -1  1  1 -1 -1  1\n84   1  1 -1 -1  1 -1  1\n90   1 -1 -1  1  1 -1  1\n123 -1  1 -1  1  1  1  1\n10   1 -1 -1  1 -1 -1 -1\n16   1  1  1  1 -1 -1 -1\n28   1  1 -1  1  1 -1 -1\n50   1 -1 -1 -1  1  1 -1\n61  -1 -1  1  1  1  1 -1\n67  -1  1 -1 -1 -1 -1  1\n86   1 -1  1 -1  1 -1  1\n100  1  1 -1 -1 -1  1  1\n5   -1 -1  1 -1 -1 -1 -1\n11  -1  1 -1  1 -1 -1 -1\n30   1 -1  1  1  1 -1 -1\n44   1  1 -1  1 -1  1 -1\n56   1  1  1 -1  1  1 -1\n66   1 -1 -1 -1 -1 -1  1\n95  -1  1  1  1  1 -1  1\n110  1 -1  1  1 -1  1  1\n38   1 -1  1 -1 -1  1 -1\n51  -1  1 -1 -1  1  1 -1\n72   1  1  1 -1 -1 -1  1\n76   1  1 -1  1 -1 -1  1\n105 -1 -1 -1  1 -1  1  1\n111 -1  1  1  1 -1  1  1\n117 -1 -1  1 -1  1  1  1\n128  1  1  1  1  1  1  1\n\n$rows\n [1]   4  17  23  33  77  84  90 123  10  16  28  50  61  67  86 100\n[17]   5  11  30  44  56  66  95 110  38  51  72  76 105 111 117 128\n\nagricolae\nagricolae is motivated by agricultural applications although the designs are applicable across a variety of fields.\n\n\nlibrary(agricolae)\n\n\n\nThe functions to create the design all begin with the word “design.” and the names of the functions are remnant of the name of the experimental design. E.g. design.rcbd generates a Randomised Complete Block Design and design.split generates a Split Plot Design.\n\n\nls(\"package:agricolae\") %>% \n  str_subset(\"^design.\")\n\n\n [1] \"design.ab\"      \"design.alpha\"   \"design.bib\"    \n [4] \"design.crd\"     \"design.cyclic\"  \"design.dau\"    \n [7] \"design.graeco\"  \"design.lattice\" \"design.lsd\"    \n[10] \"design.mat\"     \"design.rcbd\"    \"design.split\"  \n[13] \"design.strip\"   \"design.youden\" \n\nRather than going through each of the functions, I’ll just show one. The command below generates a balanced incomplete block design with 7 treatments of block size 3. This the same design structure as the first example for AlgDesign. What do you think of the input and output?\n\n\ntrt <- LETTERS[1:7]\ndesign.bib(trt = trt, k = 3)\n\n\n\nParameters BIB\n==============\nLambda     : 1\ntreatmeans : 7\nBlock size : 3\nBlocks     : 7\nReplication: 3 \n\nEfficiency factor 0.7777778 \n\n<<< Book >>>\n$parameters\n$parameters$design\n[1] \"bib\"\n\n$parameters$trt\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\"\n\n$parameters$k\n[1] 3\n\n$parameters$serie\n[1] 2\n\n$parameters$seed\n[1] 1804898417\n\n$parameters$kinds\n[1] \"Super-Duper\"\n\n\n$statistics\n       lambda treatmeans blockSize blocks r Efficiency\nvalues      1          7         3      7 3  0.7777778\n\n$sketch\n     [,1] [,2] [,3]\n[1,] \"C\"  \"B\"  \"D\" \n[2,] \"A\"  \"E\"  \"B\" \n[3,] \"F\"  \"B\"  \"G\" \n[4,] \"G\"  \"C\"  \"E\" \n[5,] \"G\"  \"A\"  \"D\" \n[6,] \"A\"  \"F\"  \"C\" \n[7,] \"F\"  \"E\"  \"D\" \n\n$book\n   plots block trt\n1    101     1   C\n2    102     1   B\n3    103     1   D\n4    201     2   A\n5    202     2   E\n6    203     2   B\n7    301     3   F\n8    302     3   B\n9    303     3   G\n10   401     4   G\n11   402     4   C\n12   403     4   E\n13   501     5   G\n14   502     5   A\n15   503     5   D\n16   601     6   A\n17   602     6   F\n18   603     6   C\n19   701     7   F\n20   702     7   E\n21   703     7   D\n\nMore examples are given in the agricolae tutorial.\nlhs\nThe lhs package is completely different to the previous two packages. It implements methods for creating and augmenting Latin Hypercube Samples and Orthogonal Array Latin Hypercube Samples. The treatment variables here are the parameters and are continuous. In the example below, there are 10 parameters were 30 samples will be drawn from.\n\n\nlibrary(lhs)\n# a design with 30 samples from 10 parameters\nA <- randomLHS(30, 10)\nA\n\n\n            [,1]       [,2]       [,3]       [,4]        [,5]\n [1,] 0.85115160 0.80153721 0.26562089 0.24240381 0.386617133\n [2,] 0.03162770 0.11851068 0.20750833 0.22137816 0.737580563\n [3,] 0.94326309 0.99286802 0.55167951 0.04431126 0.073908842\n [4,] 0.15341898 0.23664814 0.45088836 0.02736497 0.276594703\n [5,] 0.53987796 0.69129259 0.61068716 0.68112190 0.840092421\n [6,] 0.34338962 0.91067411 0.50772141 0.46340514 0.543650700\n [7,] 0.87984431 0.18530938 0.28391957 0.80767211 0.636091307\n [8,] 0.74093451 0.94142899 0.47633881 0.93482745 0.101815507\n [9,] 0.22679294 0.05950478 0.70384589 0.84840308 0.046119869\n[10,] 0.59543890 0.14230001 0.91973016 0.38743743 0.008173053\n[11,] 0.71222052 0.84574251 0.05719443 0.33460392 0.414795358\n[12,] 0.31313954 0.55023270 0.67189798 0.98743475 0.480147544\n[13,] 0.06589897 0.02727366 0.94372045 0.11200430 0.776188787\n[14,] 0.67856942 0.64694630 0.11695731 0.57667893 0.576669680\n[15,] 0.60779222 0.47346774 0.57803451 0.77359785 0.686925390\n[16,] 0.77869112 0.45967726 0.97082607 0.72001527 0.158727598\n[17,] 0.47753672 0.33965295 0.34264293 0.88078583 0.932636317\n[18,] 0.24829930 0.28054142 0.82863690 0.48987801 0.831406425\n[19,] 0.63438918 0.62665931 0.19339855 0.63126047 0.222550404\n[20,] 0.42615936 0.78697269 0.31122846 0.54731724 0.618925200\n[21,] 0.98558294 0.31283813 0.79937608 0.09708168 0.974254219\n[22,] 0.82272308 0.72620385 0.63409490 0.51439160 0.888309555\n[23,] 0.28216288 0.40678670 0.00747647 0.15786391 0.462001814\n[24,] 0.44293885 0.88759165 0.87593133 0.17614748 0.301809115\n[25,] 0.91903885 0.58769320 0.85344414 0.73340894 0.261399107\n[26,] 0.39749211 0.21531852 0.40155710 0.40795272 0.175926862\n[27,] 0.53326444 0.09494134 0.36906730 0.28462433 0.521423827\n[28,] 0.12780125 0.76041647 0.14450865 0.32753635 0.958334555\n[29,] 0.19956924 0.39260007 0.76646006 0.65147638 0.353769748\n[30,] 0.09115621 0.53145384 0.09544316 0.91317843 0.722981039\n            [,6]       [,7]       [,8]       [,9]      [,10]\n [1,] 0.57523291 0.64829514 0.30957145 0.56063376 0.58060694\n [2,] 0.37572590 0.09386871 0.09460784 0.62699580 0.07496677\n [3,] 0.33729208 0.14971910 0.02306924 0.12609065 0.18310414\n [4,] 0.77521524 0.27273750 0.43275425 0.84548534 0.13991032\n [5,] 0.18545904 0.57024842 0.83876578 0.94430739 0.55675175\n [6,] 0.04848456 0.68468958 0.93462688 0.69388363 0.77296080\n [7,] 0.40499358 0.22736413 0.49914901 0.63951785 0.29475412\n [8,] 0.83195048 0.86262149 0.90864828 0.71549074 0.95154810\n [9,] 0.20549016 0.82450164 0.34137614 0.35590469 0.33383492\n[10,] 0.11180544 0.99755568 0.77506400 0.90850387 0.04070513\n[11,] 0.71230339 0.51386141 0.51188486 0.74575618 0.47042600\n[12,] 0.14900939 0.62716610 0.60853776 0.19349115 0.85808318\n[13,] 0.31845634 0.36810057 0.68906398 0.48416759 0.10075997\n[14,] 0.68236586 0.46367072 0.53735362 0.97946521 0.53084746\n[15,] 0.46043188 0.79151449 0.39841726 0.16295411 0.32738380\n[16,] 0.52748006 0.73055037 0.10636268 0.45184454 0.60852297\n[17,] 0.08469157 0.11492100 0.64512714 0.26197028 0.87144705\n[18,] 0.28007202 0.90350371 0.27696270 0.41905694 0.25261507\n[19,] 0.84219677 0.41560144 0.73423244 0.01653787 0.01640204\n[20,] 0.98370876 0.54984313 0.26617569 0.32944947 0.20285318\n[21,] 0.48614679 0.73515250 0.59964083 0.87721937 0.82147435\n[22,] 0.94892286 0.89461948 0.05944528 0.27581653 0.97355201\n[23,] 0.89070787 0.95209334 0.17557806 0.57880924 0.65490041\n[24,] 0.03273681 0.35714952 0.82135876 0.80705301 0.91792197\n[25,] 0.65831637 0.31657015 0.44139011 0.39117389 0.72964052\n[26,] 0.54073941 0.03568867 0.88231315 0.53096409 0.45227701\n[27,] 0.62613184 0.18195056 0.23326926 0.79716759 0.42737304\n[28,] 0.25256769 0.48629095 0.72335715 0.07429013 0.67936824\n[29,] 0.90342254 0.03184096 0.15672156 0.06023826 0.73960757\n[30,] 0.74109078 0.23509647 0.98392432 0.20423196 0.38968498\n\nlhs provides a number of methods to find the optimal design each with their own criteria.\n\n\nA1 <- optimumLHS(30, 10, maxSweeps = 4, eps = 0.01)\nA2 <- maximinLHS(30, 10, dup = 5)\nA3 <- improvedLHS(30, 10, dup = 5)\nA4 <- geneticLHS(30, 10, pop = 1000, gen = 8, pMut = 0.1, criterium = \"S\")\nA5 <- geneticLHS(30, 10, pop = 1000, gen = 8, pMut = 0.1, criterium = \"Maximin\")\n\n\n\nez\nThis is mainly focussed on the analysis of experimental data but some functions such as ezDesign is useful for viewing the experimental structure.\n\n\nlibrary(ez)\ndata(ANT2)\nezPrecis(ANT2)\n\n\nData frame dimensions: 5760 rows, 10 columns\n             type missing values      min         max\nsubnum    numeric       0     20        1          20\ngroup      factor       0      2  Control   Treatment\nblock     numeric       0      6        1           6\ntrial      factor       0     48        1          48\ncue        factor       0      4     None     Spatial\nflank      factor       0      3  Neutral Incongruent\nlocation   factor       0      2     down          up\ndirection  factor       0      2     left       right\nrt        numeric     144   5617 179.5972    657.6986\nerror     numeric     144      3        0           1\n\n\n\nezDesign(data = ANT2,\n         x = trial, \n         y = subnum,\n         row = block, \n         col = group)\n\n\n\n\nDoE.base\nDoE.base provides utility functions for the special class design and as seen in Figure 1, DoE.base is used by four other DoE packages that is maintained also by Prof. Dr. Ulrike Grömping.\nDoE.base contains functions to generate factorial designs easily.\n\n\nlibrary(DoE.base)\nfac.design(nlevels = c(2, 2, 3, 3, 6), \n           blocks = 6)\n\n\n   run.no run.no.std.rp Blocks A B C D E\n1       1        29.1.5      1 1 1 2 3 1\n2       2       89.1.15      1 1 1 2 2 3\n3       3      180.1.30      1 2 2 3 3 5\n4       4         4.1.2      1 2 2 1 1 1\n5       5       84.1.14      1 2 2 3 1 3\n6       6      160.1.28      1 2 2 1 2 5\n7       7      118.1.19      1 2 1 3 1 4\n8       8      186.1.31      1 2 1 2 1 6\n9       9       97.1.17      1 1 1 1 3 3\n10     10       92.1.16      1 2 2 2 2 3\n11     11      214.1.35      1 2 1 3 3 6\n12     12      194.1.33      1 2 1 1 2 6\n13     13        39.1.8      1 1 2 1 1 2\n14     14         1.1.1      1 1 1 1 1 1\n15     15      119.1.20      1 1 2 3 1 4\n16     16      127.1.22      1 1 2 2 2 4\n17     17      134.1.23      1 2 1 1 3 4\n18     18        58.1.9      1 2 1 3 2 2\n19     19      135.1.24      1 1 2 1 3 4\n20     20      149.1.25      1 1 1 2 1 5\n21     21       59.1.10      1 1 2 3 2 2\n22     22        38.1.7      1 2 1 1 1 2\n23     23       67.1.12      1 1 2 2 3 2\n24     24      195.1.34      1 1 2 1 2 6\n25     25        32.1.6      1 2 2 2 3 1\n26     26       66.1.11      1 2 1 2 3 2\n27     27      152.1.26      1 2 2 2 1 5\n28     28        21.1.3      1 1 1 3 2 1\n29     29      157.1.27      1 1 1 1 2 5\n30     30      215.1.36      1 1 2 3 3 6\n31     31      100.1.18      1 2 2 1 3 3\n32     32      187.1.32      1 1 2 2 1 6\n33     33      177.1.29      1 1 1 3 3 5\n34     34      126.1.21      1 2 1 2 2 4\n35     35       81.1.13      1 1 1 3 1 3\n36     36        24.1.4      1 2 2 3 2 1\n   run.no run.no.std.rp Blocks A B C D E\n37     37        16.2.4      2 2 2 1 2 1\n38     38      169.2.29      2 1 1 1 3 5\n39     39        43.2.8      2 1 2 2 1 2\n40     40      199.2.34      2 1 2 2 2 6\n41     41      104.2.18      2 2 2 2 3 3\n42     42      206.2.35      2 2 1 1 3 6\n43     43      131.2.22      2 1 2 3 2 4\n44     44      138.2.23      2 2 1 2 3 4\n45     45      172.2.30      2 2 2 1 3 5\n46     46      110.2.19      2 2 1 1 1 4\n47     47      161.2.27      2 1 1 2 2 5\n48     48         5.2.1      2 1 1 2 1 1\n49     49        42.2.7      2 2 1 2 1 2\n50     50       73.2.13      2 1 1 1 1 3\n51     51      191.2.32      2 1 2 3 1 6\n52     52       93.2.15      2 1 1 3 2 3\n53     53      156.2.26      2 2 2 3 1 5\n54     54       96.2.16      2 2 2 3 2 3\n55     55       51.2.10      2 1 2 1 2 2\n56     56      101.2.17      2 1 1 2 3 3\n57     57        13.2.3      2 1 1 1 2 1\n58     58      111.2.20      2 1 2 1 1 4\n59     59      130.2.21      2 2 1 3 2 4\n60     60       76.2.14      2 2 2 1 1 3\n61     61      198.2.33      2 2 1 2 2 6\n62     62      190.2.31      2 2 1 3 1 6\n63     63        33.2.5      2 1 1 3 3 1\n64     64      153.2.25      2 1 1 3 1 5\n65     65      164.2.28      2 2 2 2 2 5\n66     66        50.2.9      2 2 1 1 2 2\n67     67      207.2.36      2 1 2 1 3 6\n68     68       71.2.12      2 1 2 3 3 2\n69     69        36.2.6      2 2 2 3 3 1\n70     70         8.2.2      2 2 2 2 1 1\n71     71       70.2.11      2 2 1 3 3 2\n72     72      139.2.24      2 1 2 2 3 4\n    run.no run.no.std.rp Blocks A B C D E\n73      73       85.3.15      3 1 1 1 2 3\n74      74      105.3.17      3 1 1 3 3 3\n75      75        17.3.3      3 1 1 2 2 1\n76      76      211.3.36      3 1 2 2 3 6\n77      77      114.3.19      3 2 1 2 1 4\n78      78        47.3.8      3 1 2 3 1 2\n79      79       55.3.10      3 1 2 2 2 2\n80      80      182.3.31      3 2 1 1 1 6\n81      81      168.3.28      3 2 2 3 2 5\n82      82      165.3.27      3 1 1 3 2 5\n83      83      142.3.23      3 2 1 3 3 4\n84      84      145.3.25      3 1 1 1 1 5\n85      85       62.3.11      3 2 1 1 3 2\n86      86      148.3.26      3 2 2 1 1 5\n87      87      108.3.18      3 2 2 3 3 3\n88      88        25.3.5      3 1 1 1 3 1\n89      89         9.3.1      3 1 1 3 1 1\n90      90       77.3.13      3 1 1 2 1 3\n91      91      122.3.21      3 2 1 1 2 4\n92      92        12.3.2      3 2 2 3 1 1\n93      93        46.3.7      3 2 1 3 1 2\n94      94       88.3.16      3 2 2 1 2 3\n95      95        20.3.4      3 2 2 2 2 1\n96      96        54.3.9      3 2 1 2 2 2\n97      97      203.3.34      3 1 2 3 2 6\n98      98       80.3.14      3 2 2 2 1 3\n99      99      123.3.22      3 1 2 1 2 4\n100    100      173.3.29      3 1 1 2 3 5\n101    101        28.3.6      3 2 2 1 3 1\n102    102      176.3.30      3 2 2 2 3 5\n103    103      202.3.33      3 2 1 3 2 6\n104    104      115.3.20      3 1 2 2 1 4\n105    105      210.3.35      3 2 1 2 3 6\n106    106      183.3.32      3 1 2 1 1 6\n107    107       63.3.12      3 1 2 1 3 2\n108    108      143.3.24      3 1 2 3 3 4\n    run.no run.no.std.rp Blocks A B C D E\n109    109      179.4.30      4 1 2 3 3 5\n110    110      151.4.26      4 1 2 2 1 5\n111    111       60.4.10      4 2 2 3 2 2\n112    112        31.4.6      4 1 2 2 3 1\n113    113         3.4.2      4 1 2 1 1 1\n114    114        22.4.3      4 2 1 3 2 1\n115    115       99.4.18      4 1 2 1 3 3\n116    116        30.4.5      4 2 1 2 3 1\n117    117       68.4.12      4 2 2 2 3 2\n118    118      196.4.34      4 2 2 1 2 6\n119    119       90.4.15      4 2 1 2 2 3\n120    120        57.4.9      4 1 1 3 2 2\n121    121      158.4.27      4 2 1 1 2 5\n122    122      193.4.33      4 1 1 1 2 6\n123    123      136.4.24      4 2 2 1 3 4\n124    124       82.4.13      4 2 1 3 1 3\n125    125       65.4.11      4 1 1 2 3 2\n126    126        37.4.7      4 1 1 1 1 2\n127    127      120.4.20      4 2 2 3 1 4\n128    128      178.4.29      4 2 1 3 3 5\n129    129      128.4.22      4 2 2 2 2 4\n130    130      188.4.32      4 2 2 2 1 6\n131    131         2.4.1      4 2 1 1 1 1\n132    132       91.4.16      4 1 2 2 2 3\n133    133      185.4.31      4 1 1 2 1 6\n134    134      159.4.28      4 1 2 1 2 5\n135    135       98.4.17      4 2 1 1 3 3\n136    136        40.4.8      4 2 2 1 1 2\n137    137      150.4.25      4 2 1 2 1 5\n138    138      125.4.21      4 1 1 2 2 4\n139    139       83.4.14      4 1 2 3 1 3\n140    140      133.4.23      4 1 1 1 3 4\n141    141        23.4.4      4 1 2 3 2 1\n142    142      117.4.19      4 1 1 3 1 4\n143    143      216.4.36      4 2 2 3 3 6\n144    144      213.4.35      4 1 1 3 3 6\n    run.no run.no.std.rp Blocks A B C D E\n145    145      171.5.30      5 1 2 1 3 5\n146    146      102.5.17      5 2 1 2 3 3\n147    147      162.5.27      5 2 1 2 2 5\n148    148      112.5.20      5 2 2 1 1 4\n149    149      154.5.25      5 2 1 3 1 5\n150    150       74.5.13      5 2 1 1 1 3\n151    151      163.5.28      5 1 2 2 2 5\n152    152        15.5.4      5 1 2 1 2 1\n153    153       72.5.12      5 2 2 3 3 2\n154    154       95.5.16      5 1 2 3 2 3\n155    155      205.5.35      5 1 1 1 3 6\n156    156        35.5.6      5 1 2 3 3 1\n157    157         7.5.2      5 1 2 2 1 1\n158    158      129.5.21      5 1 1 3 2 4\n159    159         6.5.1      5 2 1 2 1 1\n160    160       75.5.14      5 1 2 1 1 3\n161    161      208.5.36      5 2 2 1 3 6\n162    162        14.5.3      5 2 1 1 2 1\n163    163       94.5.15      5 2 1 3 2 3\n164    164      132.5.22      5 2 2 3 2 4\n165    165        34.5.5      5 2 1 3 3 1\n166    166       69.5.11      5 1 1 3 3 2\n167    167      170.5.29      5 2 1 1 3 5\n168    168      137.5.23      5 1 1 2 3 4\n169    169       52.5.10      5 2 2 1 2 2\n170    170      155.5.26      5 1 2 3 1 5\n171    171        49.5.9      5 1 1 1 2 2\n172    172      200.5.34      5 2 2 2 2 6\n173    173        41.5.7      5 1 1 2 1 2\n174    174      192.5.32      5 2 2 3 1 6\n175    175        44.5.8      5 2 2 2 1 2\n176    176      140.5.24      5 2 2 2 3 4\n177    177      197.5.33      5 1 1 2 2 6\n178    178      109.5.19      5 1 1 1 1 4\n179    179      103.5.18      5 1 2 2 3 3\n180    180      189.5.31      5 1 1 3 1 6\n    run.no run.no.std.rp Blocks A B C D E\n181    181      106.6.17      6 2 1 3 3 3\n182    182      146.6.25      6 2 1 1 1 5\n183    183       79.6.14      6 1 2 2 1 3\n184    184        53.6.9      6 1 1 2 2 2\n185    185      209.6.35      6 1 1 2 3 6\n186    186       64.6.12      6 2 2 1 3 2\n187    187      166.6.27      6 2 1 3 2 5\n188    188        19.6.4      6 1 2 2 2 1\n189    189      204.6.34      6 2 2 3 2 6\n190    190        26.6.5      6 2 1 1 3 1\n191    191       78.6.13      6 2 1 2 1 3\n192    192       56.6.10      6 2 2 2 2 2\n193    193      181.6.31      6 1 1 1 1 6\n194    194      174.6.29      6 2 1 2 3 5\n195    195       87.6.16      6 1 2 1 2 3\n196    196        10.6.1      6 2 1 3 1 1\n197    197      212.6.36      6 2 2 2 3 6\n198    198      147.6.26      6 1 2 1 1 5\n199    199      107.6.18      6 1 2 3 3 3\n200    200        48.6.8      6 2 2 3 1 2\n201    201      116.6.20      6 2 2 2 1 4\n202    202       86.6.15      6 2 1 1 2 3\n203    203      184.6.32      6 2 2 1 1 6\n204    204        27.6.6      6 1 2 1 3 1\n205    205      124.6.22      6 2 2 1 2 4\n206    206      141.6.23      6 1 1 3 3 4\n207    207      201.6.33      6 1 1 3 2 6\n208    208        18.6.3      6 2 1 2 2 1\n209    209        45.6.7      6 1 1 3 1 2\n210    210      113.6.19      6 1 1 2 1 4\n211    211      167.6.28      6 1 2 3 2 5\n212    212      121.6.21      6 1 1 1 2 4\n213    213      144.6.24      6 2 2 3 3 4\n214    214       61.6.11      6 1 1 1 3 2\n215    215      175.6.30      6 1 2 2 3 5\n216    216        11.6.2      6 1 2 3 1 1\nclass=design, type= full factorial.blocked \nNOTE: columns run.no and run.no.std.rp  are annotation, \n not part of the data frame\n\nIt also contains functions to create orthogonal array designs.\n\n\ndes <- oa.design(nlevels = c(rep(2, 8), 8))\ndes\n\n\n   A B C D E F G H J\n1  1 2 1 2 2 1 2 1 6\n2  2 1 2 1 2 1 2 1 2\n3  2 1 1 2 2 1 1 2 4\n4  2 2 1 1 2 2 1 1 3\n5  1 1 1 1 2 2 2 2 5\n6  1 1 2 2 2 2 1 1 7\n7  1 2 1 2 1 2 1 2 2\n8  1 1 1 1 1 1 1 1 1\n9  1 2 2 1 2 1 1 2 8\n10 1 1 2 2 1 1 2 2 3\n11 2 1 2 1 1 2 1 2 6\n12 2 1 1 2 1 2 2 1 8\n13 2 2 2 2 1 1 1 1 5\n14 2 2 1 1 1 1 2 2 7\n15 1 2 2 1 1 2 2 1 4\n16 2 2 2 2 2 2 2 2 1\nclass=design, type= oa \n\nIf you need to further randomise within a specified block, you can do this using rerandomize.design.\n\n\nrerandomize.design(des, block = \"J\")\n\n\n  run.no run.no.std.rp J A B C D E F G H\n1      1         4.7.1 7 1 1 2 2 2 2 1 1\n2      2        13.7.2 7 2 2 1 1 1 1 2 2\n  run.no run.no.std.rp J A B C D E F G H\n3      3         7.4.1 4 1 2 2 1 1 2 2 1\n4      4        10.4.2 4 2 1 1 2 2 1 1 2\n  run.no run.no.std.rp J A B C D E F G H\n5      5        12.2.2 2 2 1 2 1 2 1 2 1\n6      6         5.2.1 2 1 2 1 2 1 2 1 2\n  run.no run.no.std.rp J A B C D E F G H\n7      7        11.6.2 6 2 1 2 1 1 2 1 2\n8      8         6.6.1 6 1 2 1 2 2 1 2 1\n   run.no run.no.std.rp J A B C D E F G H\n9       9         9.8.2 8 2 1 1 2 1 2 2 1\n10     10         8.8.1 8 1 2 2 1 2 1 1 2\n   run.no run.no.std.rp J A B C D E F G H\n11     11         3.3.1 3 1 1 2 2 1 1 2 2\n12     12        14.3.2 3 2 2 1 1 2 2 1 1\n   run.no run.no.std.rp J A B C D E F G H\n13     13        16.1.2 1 2 2 2 2 2 2 2 2\n14     14         1.1.1 1 1 1 1 1 1 1 1 1\n   run.no run.no.std.rp J A B C D E F G H\n15     15         2.5.1 5 1 1 1 1 2 2 2 2\n16     16        15.5.2 5 2 2 2 2 1 1 1 1\nclass=design, type= oa.blocked \nNOTE: columns run.no and run.no.std.rp  are annotation, \n not part of the data frame\n\nSo those were the top 5 DoE packages. The API of the packages are quite distinct. The object that it outputs can vary from a matrix to a list. DoE might be a dull area for many but it’s quite important for the downstream analysis. Perhaps if many of us talk more about it, it may help invigorate the area!\n\n\nfunction showscript() {\n  var x = document.getElementsByClassName('toggle-code');\n  var n = x.length;\n  for (var i = 0; i < n; i++) {\n    var e = x[i];\n    if (e.style.display == \"block\") {\n      e.style.display = \"none\";\n    } else {\n      e.style.display = \"block\";\n    }\n  }\n}\n\n\n\n\nCarnell, Rob. 2020. Lhs: Latin Hypercube Samples. https://CRAN.R-project.org/package=lhs.\n\n\nChatfield, C. 1985. “The Initial Examination of Data.” Journal of the Royal Statistical Society. Series A 148 (3): 214–53.\n\n\nde Mendiburu, Felipe. 2020. Agricolae: Statistical Procedures for Agricultural Research. https://CRAN.R-project.org/package=agricolae.\n\n\nGrömping, Ulrike. 2018. “R Package DoE.base for Factorial Experiments.” Journal of Statistical Software 85 (5): 1–41. https://doi.org/10.18637/jss.v085.i05.\n\n\nLawrence, Michael A. 2016. Ez: Easy Analysis and Visualization of Factorial Experiments. https://CRAN.R-project.org/package=ez.\n\n\nMorgan-Wall, Tyler. 2017. Adjustedcranlogs: Remove Automated and Repeated Downloads from ’Rstudio’ ’Cran’ Download Logs. https://CRAN.R-project.org/package=adjustedcranlogs.\n\n\nWheeler, Bob. 2019. AlgDesign: Algorithmic Experimental Design. https://CRAN.R-project.org/package=AlgDesign.\n\n\nAt least from my teaching experience, statistics subjects are primary about the analysis and most research grants I’ve seen are about an analytical method. The analytical focus is reflected also in the R packages; there are 1,779 R-packages on CRAN with the word “analysis” in the title as opposed to 257 R-packages with the word “design” in its title.↩︎\nKeeping in mind though that your analysis plan may change once you actually have collected data. This is quite common in the analysis of plant breeding trials since some spatial variation only become apparent only after the data collection.↩︎\nI originally had a webscrapping error where I didn’t remove duplicate entries so numbers presented at TokyoR and SSA Webinar had the wrong numbers.↩︎\nAs of 2021-02-07.↩︎\n",
    "preview": "posts/2021-02-03-current-state-of-experimental-design-r-packages/figures/download-timeplot-1.png",
    "last_modified": "2021-05-18T16:21:11+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1344
  },
  {
    "path": "posts/2021-06-17-islrtreebasedmethods/",
    "title": "ISLR Machine Learning ",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Jixing Liu",
        "url": "https://emitanaka.org"
      }
    ],
    "date": "2021-02-03",
    "categories": [
      "machine learning",
      "R"
    ],
    "contents": "\n\nContents\nload pkg\nload data\nsplit data\n🐍 module\nr data to python\npycaret setup\nauto ML\nmodel compare\ncreate and tune model\nextract data\nplot metrics\n📌 model predict\nmodel save\n\nmodel load\n\n\nload pkg\nload data\n\n\n\nlibrary(ISLR)\nlibrary(MASS)\nlibrary(tidymodels)\n# Boston <- as_tibble(Boston)\nCarseats <- as_tibble(Carseats) %>%\n  mutate(High = factor(if_else(Sales <= 8, \"No\", \"Yes\")))\n\ndata <- \nCarseats %>% \n  dplyr::select(-Sales)\n\ndata <- data %>% \n  dplyr::mutate(\n    High = as.factor(High),\n    High = fct_collapse(High,\n                        `0` = c(\"No\"),\n                        `1` = c(\"Yes\")),\n    High = fct_relevel(High,\n                       \"0\",\n                       \"1\"))\n\n\n\n\nsplit data\n\n\n\nset.seed(123)\ndata_split <- initial_split(data, prop = 0.7)\n\ndata_train <- training(data_split)\ndata_test <- testing(data_split)\n\n\n\n\n🐍 module\n\n\nimport IPython\nfrom IPython.display import HTML, display, Markdown, IFrame\n\nfrom pycaret.classification import *\nimport pandas as pd               #data loading and manipulation\nimport matplotlib.pyplot as plt   #plotting\nimport seaborn as sns             #statistical plotting\n\n\nr data to python\n\n\ntrain_data = r.data_train\ntest_data = r.data_test\ntrain_data.head()\n   CompPrice  Income  Advertising  Population  Price ShelveLoc   Age  \\\n0      104.0    71.0         14.0        89.0   81.0    Medium  25.0   \n1      115.0    28.0         11.0        29.0   86.0      Good  53.0   \n2      112.0    98.0         18.0       481.0  128.0    Medium  45.0   \n3      115.0    29.0         26.0       394.0  132.0    Medium  33.0   \n4      145.0    53.0          0.0       507.0  119.0    Medium  41.0   \n\n   Education Urban   US High  \n0       14.0    No  Yes    1  \n1       18.0   Yes  Yes    1  \n2       11.0   Yes  Yes    0  \n3       13.0   Yes  Yes    1  \n4       12.0   Yes   No    1  \n\n\npycaret setup\nFunctions - PyCaret\n\n\nclf1 = setup(data=train_data, \n             #data_split_shuffle = False,\n             #ignore_features=[\"ID\"],\n             target = \"High\",\n             train_size = 0.7,\n             feature_selection = True,\n             remove_multicollinearity = True,\n             normalize=True,\n             ignore_low_variance=True,\n             session_id=123,\n             html = False, silent=True\n            )  \n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nSetup Succesfully Completed!\n                               Description             Value\n0                               session_id               123\n1                                   Target              High\n2                              Target Type            Binary\n3                            Label Encoded        0: 0, 1: 1\n4                            Original Data         (280, 11)\n5                           Missing Values             False\n6                         Numeric Features                 7\n7                     Categorical Features                 3\n8                         Ordinal Features             False\n9                High Cardinality Features             False\n10                 High Cardinality Method              None\n11                   Transformed Train Set         (195, 11)\n12                    Transformed Test Set          (85, 11)\n13                      Shuffle Train-Test              True\n14                     Stratify Train-Test             False\n15                          Fold Generator   StratifiedKFold\n16                             Fold Number                10\n17                                CPU Jobs                -1\n18                                 Use GPU             False\n19                          Log Experiment             False\n20                         Experiment Name  clf-default-name\n21                                     USI              2891\n22                         Imputation Type            simple\n23          Iterative Imputation Iteration              None\n24                         Numeric Imputer              mean\n25      Iterative Imputation Numeric Model              None\n26                     Categorical Imputer          constant\n27  Iterative Imputation Categorical Model              None\n28           Unknown Categoricals Handling    least_frequent\n29                               Normalize              True\n30                        Normalize Method            zscore\n31                          Transformation             False\n32                   Transformation Method              None\n33                                     PCA             False\n34                              PCA Method              None\n35                          PCA Components              None\n36                     Ignore Low Variance              True\n37                     Combine Rare Levels             False\n38                    Rare Level Threshold              None\n39                         Numeric Binning             False\n40                         Remove Outliers             False\n41                      Outliers Threshold              None\n42                Remove Multicollinearity              True\n43             Multicollinearity Threshold               0.9\n44                              Clustering             False\n45                    Clustering Iteration              None\n46                     Polynomial Features             False\n47                       Polynomial Degree              None\n48                    Trignometry Features             False\n49                    Polynomial Threshold              None\n50                          Group Features             False\n51                       Feature Selection              True\n52                Feature Selection Method           classic\n53            Features Selection Threshold               0.8\n54                     Feature Interaction             False\n55                           Feature Ratio             False\n56                   Interaction Threshold              None\n57                           Fix Imbalance             False\n58                    Fix Imbalance Method             SMOTE\n\n\nauto ML\n\n\n# compare all baseline models and select top 5\ntop5 = compare_models(n_select = 5) \n# tune top 5 base models\n                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\nlr  Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452  0.7206   \n\n       MCC  TT (Sec)  \nlr  0.7312     0.373  \n                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\nlr      Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452  0.7206   \nknn  K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559  0.4829   \n\n        MCC  TT (Sec)  \nlr   0.7312     0.373  \nknn  0.4957     0.049  \n                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\nlr      Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452  0.7206   \nknn  K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559  0.4829   \nnb              Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709  0.4815   \n\n        MCC  TT (Sec)  \nlr   0.7312     0.373  \nknn  0.4957     0.049  \nnb   0.4964     0.006  \n                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\nlr        Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452   \ndt   Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774  0.7265   \nknn    K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559   \nnb                Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709   \n\n      Kappa     MCC  TT (Sec)  \nlr   0.7206  0.7312     0.373  \ndt   0.5460  0.5579     0.006  \nknn  0.4829  0.4957     0.049  \nnb   0.4815  0.4964     0.006  \n                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\nlr        Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452   \nsvm       SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946  0.8029   \ndt   Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774  0.7265   \nknn    K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559   \nnb                Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709   \n\n      Kappa     MCC  TT (Sec)  \nlr   0.7206  0.7312     0.373  \nsvm  0.6464  0.6537     0.006  \ndt   0.5460  0.5579     0.006  \nknn  0.4829  0.4957     0.049  \nnb   0.4815  0.4964     0.006  \n                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\nridge          Ridge Classifier    0.8771  0.0000  0.8819  0.8498  0.8604   \nlr          Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452   \nsvm         SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946  0.8029   \ndt     Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774  0.7265   \nknn      K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559   \nnb                  Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709   \n\n        Kappa     MCC  TT (Sec)  \nridge  0.7507  0.7590     0.005  \nlr     0.7206  0.7312     0.373  \nsvm    0.6464  0.6537     0.006  \ndt     0.5460  0.5579     0.006  \nknn    0.4829  0.4957     0.049  \nnb     0.4815  0.4964     0.006  \n                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\nridge          Ridge Classifier    0.8771  0.0000  0.8819  0.8498  0.8604   \nlr          Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452   \nsvm         SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946  0.8029   \nrf     Random Forest Classifier    0.7895  0.8856  0.7486  0.7903  0.7479   \ndt     Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774  0.7265   \nknn      K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559   \nnb                  Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709   \n\n        Kappa     MCC  TT (Sec)  \nridge  0.7507  0.7590     0.005  \nlr     0.7206  0.7312     0.373  \nsvm    0.6464  0.6537     0.006  \nrf     0.5691  0.5910     0.197  \ndt     0.5460  0.5579     0.006  \nknn    0.4829  0.4957     0.049  \nnb     0.4815  0.4964     0.006  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \n\n           F1   Kappa     MCC  TT (Sec)  \nridge  0.8604  0.7507  0.7590     0.005  \nlr     0.8452  0.7206  0.7312     0.373  \nsvm    0.8029  0.6464  0.6537     0.006  \nrf     0.7479  0.5691  0.5910     0.197  \ndt     0.7265  0.5460  0.5579     0.006  \nknn    0.6559  0.4829  0.4957     0.049  \nnb     0.6709  0.4815  0.4964     0.006  \nqda    0.4319  0.2688  0.3032     0.009  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nada               Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \n\n           F1   Kappa     MCC  TT (Sec)  \nridge  0.8604  0.7507  0.7590     0.005  \nlr     0.8452  0.7206  0.7312     0.373  \nsvm    0.8029  0.6464  0.6537     0.006  \nada    0.7543  0.5864  0.5970     0.034  \nrf     0.7479  0.5691  0.5910     0.197  \ndt     0.7265  0.5460  0.5579     0.006  \nknn    0.6559  0.4829  0.4957     0.049  \nnb     0.6709  0.4815  0.4964     0.006  \nqda    0.4319  0.2688  0.3032     0.009  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \ngbc       Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nada               Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \n\n           F1   Kappa     MCC  TT (Sec)  \nridge  0.8604  0.7507  0.7590     0.005  \nlr     0.8452  0.7206  0.7312     0.373  \nsvm    0.8029  0.6464  0.6537     0.006  \ngbc    0.7719  0.5952  0.6137     0.033  \nada    0.7543  0.5864  0.5970     0.034  \nrf     0.7479  0.5691  0.5910     0.197  \ndt     0.7265  0.5460  0.5579     0.006  \nknn    0.6559  0.4829  0.4957     0.049  \nnb     0.6709  0.4815  0.4964     0.006  \nqda    0.4319  0.2688  0.3032     0.009  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nlda       Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \ngbc       Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nada               Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \n\n           F1   Kappa     MCC  TT (Sec)  \nlda    0.8656  0.7609  0.7680     0.006  \nridge  0.8604  0.7507  0.7590     0.005  \nlr     0.8452  0.7206  0.7312     0.373  \nsvm    0.8029  0.6464  0.6537     0.006  \ngbc    0.7719  0.5952  0.6137     0.033  \nada    0.7543  0.5864  0.5970     0.034  \nrf     0.7479  0.5691  0.5910     0.197  \ndt     0.7265  0.5460  0.5579     0.006  \nknn    0.6559  0.4829  0.4957     0.049  \nnb     0.6709  0.4815  0.4964     0.006  \nqda    0.4319  0.2688  0.3032     0.009  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nlda       Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \ngbc       Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nada               Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \net              Extra Trees Classifier    0.7797  0.8659  0.6528  0.8024   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \n\n           F1   Kappa     MCC  TT (Sec)  \nlda    0.8656  0.7609  0.7680     0.006  \nridge  0.8604  0.7507  0.7590     0.005  \nlr     0.8452  0.7206  0.7312     0.373  \nsvm    0.8029  0.6464  0.6537     0.006  \ngbc    0.7719  0.5952  0.6137     0.033  \nada    0.7543  0.5864  0.5970     0.034  \nrf     0.7479  0.5691  0.5910     0.197  \ndt     0.7265  0.5460  0.5579     0.006  \net     0.7003  0.5343  0.5540     0.186  \nknn    0.6559  0.4829  0.4957     0.049  \nnb     0.6709  0.4815  0.4964     0.006  \nqda    0.4319  0.2688  0.3032     0.009  \n                                   Model  Accuracy     AUC  Recall   Prec.  \\\nlda         Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nridge                   Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \nlr                   Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nsvm                  SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \ngbc         Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nada                 Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nxgboost        Extreme Gradient Boosting    0.8000  0.8861  0.7722  0.7809   \nrf              Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \ndt              Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \net                Extra Trees Classifier    0.7797  0.8659  0.6528  0.8024   \nknn               K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nnb                           Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nqda      Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \n\n             F1   Kappa     MCC  TT (Sec)  \nlda      0.8656  0.7609  0.7680     0.006  \nridge    0.8604  0.7507  0.7590     0.005  \nlr       0.8452  0.7206  0.7312     0.373  \nsvm      0.8029  0.6464  0.6537     0.006  \ngbc      0.7719  0.5952  0.6137     0.033  \nada      0.7543  0.5864  0.5970     0.034  \nxgboost  0.7672  0.5919  0.6025     0.041  \nrf       0.7479  0.5691  0.5910     0.197  \ndt       0.7265  0.5460  0.5579     0.006  \net       0.7003  0.5343  0.5540     0.186  \nknn      0.6559  0.4829  0.4957     0.049  \nnb       0.6709  0.4815  0.4964     0.006  \nqda      0.4319  0.2688  0.3032     0.009  \n                                    Model  Accuracy     AUC  Recall   Prec.  \\\nlda          Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nridge                    Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \nlr                    Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nsvm                   SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \ngbc          Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nada                  Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nxgboost         Extreme Gradient Boosting    0.8000  0.8861  0.7722  0.7809   \nlightgbm  Light Gradient Boosting Machine    0.7897  0.8789  0.7708  0.7661   \nrf               Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \ndt               Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \net                 Extra Trees Classifier    0.7797  0.8659  0.6528  0.8024   \nknn                K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nnb                            Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nqda       Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \n\n              F1   Kappa     MCC  TT (Sec)  \nlda       0.8656  0.7609  0.7680     0.006  \nridge     0.8604  0.7507  0.7590     0.005  \nlr        0.8452  0.7206  0.7312     0.373  \nsvm       0.8029  0.6464  0.6537     0.006  \ngbc       0.7719  0.5952  0.6137     0.033  \nada       0.7543  0.5864  0.5970     0.034  \nxgboost   0.7672  0.5919  0.6025     0.041  \nlightgbm  0.7525  0.5700  0.5865     0.020  \nrf        0.7479  0.5691  0.5910     0.197  \ndt        0.7265  0.5460  0.5579     0.006  \net        0.7003  0.5343  0.5540     0.186  \nknn       0.6559  0.4829  0.4957     0.049  \nnb        0.6709  0.4815  0.4964     0.006  \nqda       0.4319  0.2688  0.3032     0.009  \n                                    Model  Accuracy     AUC  Recall   Prec.  \\\nlda          Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nridge                    Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \nlr                    Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nsvm                   SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \ngbc          Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nada                  Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nxgboost         Extreme Gradient Boosting    0.8000  0.8861  0.7722  0.7809   \nlightgbm  Light Gradient Boosting Machine    0.7897  0.8789  0.7708  0.7661   \nrf               Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \ndt               Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \net                 Extra Trees Classifier    0.7797  0.8659  0.6528  0.8024   \nknn                K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nnb                            Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nqda       Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \n\n              F1   Kappa     MCC  TT (Sec)  \nlda       0.8656  0.7609  0.7680     0.006  \nridge     0.8604  0.7507  0.7590     0.005  \nlr        0.8452  0.7206  0.7312     0.373  \nsvm       0.8029  0.6464  0.6537     0.006  \ngbc       0.7719  0.5952  0.6137     0.033  \nada       0.7543  0.5864  0.5970     0.034  \nxgboost   0.7672  0.5919  0.6025     0.041  \nlightgbm  0.7525  0.5700  0.5865     0.020  \nrf        0.7479  0.5691  0.5910     0.197  \ndt        0.7265  0.5460  0.5579     0.006  \net        0.7003  0.5343  0.5540     0.186  \nknn       0.6559  0.4829  0.4957     0.049  \nnb        0.6709  0.4815  0.4964     0.006  \nqda       0.4319  0.2688  0.3032     0.009  \ntuned_top5 = [tune_model(i) for i in top5]\n# ensemble top 5 tuned models\nFitting 10 folds for each of 10 candidates, totalling 100 fits\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.9000  0.9792  0.7500  1.0000  0.8571  0.7826  0.8018\n1       0.8500  0.9596  0.8889  0.8000  0.8421  0.7000  0.7035\n2       0.9500  0.9596  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.7500  0.9394  0.5556  0.8333  0.6667  0.4792  0.5044\n4       0.8500  0.8990  0.8889  0.8000  0.8421  0.7000  0.7035\n5       0.8421  0.9205  0.8750  0.7778  0.8235  0.6816  0.6854\n6       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n7       0.7895  0.8409  0.7500  0.7500  0.7500  0.5682  0.5682\n8       0.9474  0.9886  1.0000  0.8889  0.9412  0.8939  0.8989\n9       0.8947  0.9091  0.7500  1.0000  0.8571  0.7765  0.7966\nMean    0.8774  0.9396  0.8347  0.8850  0.8521  0.7480  0.7565\nSD      0.0726  0.0459  0.1283  0.0999  0.0914  0.1493  0.1464\nFitting 10 folds for each of 10 candidates, totalling 100 fits\n      Accuracy  AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.8500  0.0  0.7500  0.8571  0.8000  0.6809  0.6847\n1       0.9000  0.0  0.8889  0.8889  0.8889  0.7980  0.7980\n2       0.9500  0.0  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.8000  0.0  0.6667  0.8571  0.7500  0.5876  0.6005\n4       0.8000  0.0  0.8889  0.7273  0.8000  0.6040  0.6162\n5       0.8947  0.0  1.0000  0.8000  0.8889  0.7912  0.8090\n6       0.9474  0.0  1.0000  0.8889  0.9412  0.8939  0.8989\n7       0.7895  0.0  0.7500  0.7500  0.7500  0.5682  0.5682\n8       0.9474  0.0  1.0000  0.8889  0.9412  0.8939  0.8989\n9       0.9474  0.0  0.8750  1.0000  0.9333  0.8902  0.8956\nMean    0.8826  0.0  0.8708  0.8658  0.8635  0.7606  0.7673\nSD      0.0640  0.0  0.1104  0.0864  0.0762  0.1309  0.1301\nFitting 10 folds for each of 10 candidates, totalling 100 fits\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.8500  0.9792  0.7500  0.8571  0.8000  0.6809  0.6847\n1       0.8000  0.9697  0.8889  0.7273  0.8000  0.6040  0.6162\n2       0.9500  0.9798  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.9000  0.9394  0.8889  0.8889  0.8889  0.7980  0.7980\n4       0.8000  0.9394  0.8889  0.7273  0.8000  0.6040  0.6162\n5       0.8421  0.9318  1.0000  0.7273  0.8421  0.6919  0.7273\n6       0.9474  0.9886  1.0000  0.8889  0.9412  0.8939  0.8989\n7       0.7895  0.8636  0.8750  0.7000  0.7778  0.5824  0.5955\n8       0.8947  0.9886  1.0000  0.8000  0.8889  0.7912  0.8090\n9       0.9474  0.9091  0.8750  1.0000  0.9333  0.8902  0.8956\nMean    0.8721  0.9489  0.9056  0.8317  0.8613  0.7434  0.7544\nSD      0.0611  0.0385  0.0734  0.1072  0.0617  0.1203  0.1169\nFitting 10 folds for each of 10 candidates, totalling 100 fits\n      Accuracy  AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.8500  0.0  0.7500  0.8571  0.8000  0.6809  0.6847\n1       0.9000  0.0  0.8889  0.8889  0.8889  0.7980  0.7980\n2       0.9000  0.0  0.7778  1.0000  0.8750  0.7938  0.8112\n3       0.8500  0.0  0.7778  0.8750  0.8235  0.6939  0.6975\n4       0.8000  0.0  0.8889  0.7273  0.8000  0.6040  0.6162\n5       0.9474  0.0  1.0000  0.8889  0.9412  0.8939  0.8989\n6       0.8421  0.0  0.8750  0.7778  0.8235  0.6816  0.6854\n7       0.7895  0.0  0.7500  0.7500  0.7500  0.5682  0.5682\n8       0.8947  0.0  1.0000  0.8000  0.8889  0.7912  0.8090\n9       0.8947  0.0  0.7500  1.0000  0.8571  0.7765  0.7966\nMean    0.8668  0.0  0.8458  0.8565  0.8448  0.7282  0.7366\nSD      0.0468  0.0  0.0943  0.0900  0.0531  0.0946  0.0971\nFitting 10 folds for each of 10 candidates, totalling 100 fits\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.8000  0.8958  0.7500  0.7500  0.7500  0.5833  0.5833\n1       0.8000  0.9596  0.8889  0.7273  0.8000  0.6040  0.6162\n2       0.8500  0.9697  0.7778  0.8750  0.8235  0.6939  0.6975\n3       0.9000  0.9596  0.7778  1.0000  0.8750  0.7938  0.8112\n4       0.9000  0.9394  0.8889  0.8889  0.8889  0.7980  0.7980\n5       0.8947  0.9205  0.8750  0.8750  0.8750  0.7841  0.7841\n6       0.7368  0.8523  0.8750  0.6364  0.7368  0.4865  0.5114\n7       0.6842  0.7614  0.5000  0.6667  0.5714  0.3294  0.3380\n8       0.8947  0.9432  1.0000  0.8000  0.8889  0.7912  0.8090\n9       0.8421  0.8523  0.6250  1.0000  0.7692  0.6587  0.7008\nMean    0.8303  0.9054  0.7958  0.8219  0.7979  0.6523  0.6649\nSD      0.0710  0.0627  0.1382  0.1211  0.0934  0.1478  0.1470\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.4s finished\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.6s finished\nbagged_top5 = [ensemble_model(i) for i in tuned_top5]\n# blend top 5 base models \n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.8500  0.9792  0.7500  0.8571  0.8000  0.6809  0.6847\n1       0.7500  0.9596  0.8889  0.6667  0.7619  0.5098  0.5334\n2       0.9500  0.9798  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.8000  0.9394  0.6667  0.8571  0.7500  0.5876  0.6005\n4       0.8500  0.9293  0.8889  0.8000  0.8421  0.7000  0.7035\n5       0.8947  0.9205  0.8750  0.8750  0.8750  0.7841  0.7841\n6       0.8947  0.9886  0.8750  0.8750  0.8750  0.7841  0.7841\n7       0.7895  0.8182  0.7500  0.7500  0.7500  0.5682  0.5682\n8       0.8947  0.9886  0.8750  0.8750  0.8750  0.7841  0.7841\n9       0.8947  0.9091  0.7500  1.0000  0.8571  0.7765  0.7966\nMean    0.8568  0.9412  0.8208  0.8556  0.8327  0.7073  0.7142\nSD      0.0581  0.0495  0.0784  0.0963  0.0613  0.1150  0.1118\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.9000  0.9740  0.8750  0.8750  0.8750  0.7917  0.7917\n1       0.8000  0.9596  0.8889  0.7273  0.8000  0.6040  0.6162\n2       0.9500  0.9899  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.8500  0.8788  0.7778  0.8750  0.8235  0.6939  0.6975\n4       0.8500  0.8384  0.8889  0.8000  0.8421  0.7000  0.7035\n5       0.8947  0.9432  0.8750  0.8750  0.8750  0.7841  0.7841\n6       0.9474  0.9886  1.0000  0.8889  0.9412  0.8939  0.8989\n7       0.8421  0.8182  0.7500  0.8571  0.8000  0.6705  0.6746\n8       0.9474  0.9886  0.8750  1.0000  0.9333  0.8902  0.8956\n9       0.9474  0.9375  0.8750  1.0000  0.9333  0.8902  0.8956\nMean    0.8929  0.9317  0.8694  0.8898  0.8765  0.7816  0.7860\nSD      0.0521  0.0608  0.0640  0.0852  0.0552  0.1040  0.1031\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.9000  0.9792  0.8750  0.8750  0.8750  0.7917  0.7917\n1       0.7500  0.9596  0.8889  0.6667  0.7619  0.5098  0.5334\n2       0.9000  0.9798  0.8889  0.8889  0.8889  0.7980  0.7980\n3       0.9000  0.9394  0.8889  0.8889  0.8889  0.7980  0.7980\n4       0.8500  0.9394  0.8889  0.8000  0.8421  0.7000  0.7035\n5       0.8947  0.9545  1.0000  0.8000  0.8889  0.7912  0.8090\n6       0.9474  0.9773  1.0000  0.8889  0.9412  0.8939  0.8989\n7       0.7895  0.8523  0.8750  0.7000  0.7778  0.5824  0.5955\n8       0.8947  0.9886  1.0000  0.8000  0.8889  0.7912  0.8090\n9       0.8947  0.9205  0.7500  1.0000  0.8571  0.7765  0.7966\nMean    0.8721  0.9491  0.9056  0.8308  0.8611  0.7433  0.7534\nSD      0.0564  0.0384  0.0734  0.0936  0.0518  0.1091  0.1051\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.8500  0.9271  0.7500  0.8571  0.8000  0.6809  0.6847\n1       0.8500  0.9242  1.0000  0.7500  0.8571  0.7059  0.7385\n2       0.9500  0.9798  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.8000  0.8384  0.6667  0.8571  0.7500  0.5876  0.6005\n4       0.7500  0.8990  0.7778  0.7000  0.7368  0.5000  0.5025\n5       0.8947  0.9432  1.0000  0.8000  0.8889  0.7912  0.8090\n6       0.8947  0.9375  0.8750  0.8750  0.8750  0.7841  0.7841\n7       0.7895  0.8125  0.7500  0.7500  0.7500  0.5682  0.5682\n8       0.8947  0.9830  0.8750  0.8750  0.8750  0.7841  0.7841\n9       0.8947  0.9318  0.7500  1.0000  0.8571  0.7765  0.7966\nMean    0.8568  0.9176  0.8333  0.8464  0.8331  0.7076  0.7171\nSD      0.0581  0.0521  0.1067  0.0955  0.0661  0.1172  0.1190\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.7500  0.8958  0.6250  0.7143  0.6667  0.4681  0.4708\n1       0.8000  0.8687  0.8889  0.7273  0.8000  0.6040  0.6162\n2       0.8000  0.9596  0.6667  0.8571  0.7500  0.5876  0.6005\n3       0.8000  0.9596  0.6667  0.8571  0.7500  0.5876  0.6005\n4       0.8000  0.8788  0.8889  0.7273  0.8000  0.6040  0.6162\n5       0.8421  0.9091  0.7500  0.8571  0.8000  0.6705  0.6746\n6       0.7895  0.9091  0.7500  0.7500  0.7500  0.5682  0.5682\n7       0.7368  0.7727  0.6250  0.7143  0.6667  0.4509  0.4536\n8       0.7895  0.9432  0.7500  0.7500  0.7500  0.5682  0.5682\n9       0.7895  0.9091  0.6250  0.8333  0.7143  0.5529  0.5673\nMean    0.7897  0.9006  0.7236  0.7788  0.7448  0.5662  0.5736\nSD      0.0275  0.0518  0.0961  0.0606  0.0473  0.0615  0.0635\nblender = blend_models(estimator_list = top5) \n# select best model \n      Accuracy  AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.8500  0.0  0.7500  0.8571  0.8000  0.6809  0.6847\n1       0.9000  0.0  0.8889  0.8889  0.8889  0.7980  0.7980\n2       0.9500  0.0  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.8500  0.0  0.7778  0.8750  0.8235  0.6939  0.6975\n4       0.8000  0.0  0.8889  0.7273  0.8000  0.6040  0.6162\n5       0.8947  0.0  1.0000  0.8000  0.8889  0.7912  0.8090\n6       0.8947  0.0  1.0000  0.8000  0.8889  0.7912  0.8090\n7       0.7895  0.0  0.7500  0.7500  0.7500  0.5682  0.5682\n8       0.8947  0.0  1.0000  0.8000  0.8889  0.7912  0.8090\n9       0.9474  0.0  0.8750  1.0000  0.9333  0.8902  0.8956\nMean    0.8771  0.0  0.8819  0.8498  0.8604  0.7507  0.7590\nSD      0.0518  0.0  0.0936  0.0893  0.0599  0.1055  0.1070\nbest = automl(optimize = 'Recall')\n\n\nmodel compare\n\n\nmodels()\n# 比较所有模型\n                                     Name  ...  Turbo\nID                                         ...       \nlr                    Logistic Regression  ...   True\nknn                K Neighbors Classifier  ...   True\nnb                            Naive Bayes  ...   True\ndt               Decision Tree Classifier  ...   True\nsvm                   SVM - Linear Kernel  ...   True\nrbfsvm                SVM - Radial Kernel  ...  False\ngpc           Gaussian Process Classifier  ...  False\nmlp                        MLP Classifier  ...  False\nridge                    Ridge Classifier  ...   True\nrf               Random Forest Classifier  ...   True\nqda       Quadratic Discriminant Analysis  ...   True\nada                  Ada Boost Classifier  ...   True\ngbc          Gradient Boosting Classifier  ...   True\nlda          Linear Discriminant Analysis  ...   True\net                 Extra Trees Classifier  ...   True\nxgboost         Extreme Gradient Boosting  ...   True\nlightgbm  Light Gradient Boosting Machine  ...   True\n\n[17 rows x 3 columns]\nres_compare_models = compare_models(sort = \"AUC\")\n                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\nlr  Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452  0.7206   \n\n       MCC  TT (Sec)  \nlr  0.7312     0.008  \n                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\nlr      Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452  0.7206   \nknn  K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559  0.4829   \n\n        MCC  TT (Sec)  \nlr   0.7312     0.008  \nknn  0.4957     0.047  \n                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\nlr      Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452  0.7206   \nnb              Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709  0.4815   \nknn  K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559  0.4829   \n\n        MCC  TT (Sec)  \nlr   0.7312     0.008  \nnb   0.4964     0.007  \nknn  0.4957     0.047  \n                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\nlr        Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452   \nnb                Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709   \nknn    K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559   \ndt   Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774  0.7265   \n\n      Kappa     MCC  TT (Sec)  \nlr   0.7206  0.7312     0.008  \nnb   0.4815  0.4964     0.007  \nknn  0.4829  0.4957     0.047  \ndt   0.5460  0.5579     0.007  \n                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\nlr        Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452   \nnb                Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709   \nknn    K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559   \ndt   Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774  0.7265   \nsvm       SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946  0.8029   \n\n      Kappa     MCC  TT (Sec)  \nlr   0.7206  0.7312     0.008  \nnb   0.4815  0.4964     0.007  \nknn  0.4829  0.4957     0.047  \ndt   0.5460  0.5579     0.007  \nsvm  0.6464  0.6537     0.006  \n                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\nlr          Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452   \nnb                  Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709   \nknn      K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559   \ndt     Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774  0.7265   \nsvm         SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946  0.8029   \nridge          Ridge Classifier    0.8771  0.0000  0.8819  0.8498  0.8604   \n\n        Kappa     MCC  TT (Sec)  \nlr     0.7206  0.7312     0.008  \nnb     0.4815  0.4964     0.007  \nknn    0.4829  0.4957     0.047  \ndt     0.5460  0.5579     0.007  \nsvm    0.6464  0.6537     0.006  \nridge  0.7507  0.7590     0.006  \n                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\nlr          Logistic Regression    0.8618  0.9557  0.8694  0.8362  0.8452   \nrf     Random Forest Classifier    0.7895  0.8856  0.7486  0.7903  0.7479   \nnb                  Naive Bayes    0.7534  0.8437  0.6181  0.7633  0.6709   \nknn      K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663  0.6559   \ndt     Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774  0.7265   \nsvm         SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946  0.8029   \nridge          Ridge Classifier    0.8771  0.0000  0.8819  0.8498  0.8604   \n\n        Kappa     MCC  TT (Sec)  \nlr     0.7206  0.7312     0.008  \nrf     0.5691  0.5910     0.209  \nnb     0.4815  0.4964     0.007  \nknn    0.4829  0.4957     0.047  \ndt     0.5460  0.5579     0.007  \nsvm    0.6464  0.6537     0.006  \nridge  0.7507  0.7590     0.006  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \n\n           F1   Kappa     MCC  TT (Sec)  \nlr     0.8452  0.7206  0.7312     0.008  \nrf     0.7479  0.5691  0.5910     0.209  \nnb     0.6709  0.4815  0.4964     0.007  \nknn    0.6559  0.4829  0.4957     0.047  \nqda    0.4319  0.2688  0.3032     0.008  \ndt     0.7265  0.5460  0.5579     0.007  \nsvm    0.8029  0.6464  0.6537     0.006  \nridge  0.8604  0.7507  0.7590     0.006  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \nada               Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \n\n           F1   Kappa     MCC  TT (Sec)  \nlr     0.8452  0.7206  0.7312     0.008  \nrf     0.7479  0.5691  0.5910     0.209  \nada    0.7543  0.5864  0.5970     0.038  \nnb     0.6709  0.4815  0.4964     0.007  \nknn    0.6559  0.4829  0.4957     0.047  \nqda    0.4319  0.2688  0.3032     0.008  \ndt     0.7265  0.5460  0.5579     0.007  \nsvm    0.8029  0.6464  0.6537     0.006  \nridge  0.8604  0.7507  0.7590     0.006  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \ngbc       Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \nada               Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \n\n           F1   Kappa     MCC  TT (Sec)  \nlr     0.8452  0.7206  0.7312     0.008  \ngbc    0.7719  0.5952  0.6137     0.032  \nrf     0.7479  0.5691  0.5910     0.209  \nada    0.7543  0.5864  0.5970     0.038  \nnb     0.6709  0.4815  0.4964     0.007  \nknn    0.6559  0.4829  0.4957     0.047  \nqda    0.4319  0.2688  0.3032     0.008  \ndt     0.7265  0.5460  0.5579     0.007  \nsvm    0.8029  0.6464  0.6537     0.006  \nridge  0.8604  0.7507  0.7590     0.006  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nlda       Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \ngbc       Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \nada               Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \n\n           F1   Kappa     MCC  TT (Sec)  \nlda    0.8656  0.7609  0.7680     0.007  \nlr     0.8452  0.7206  0.7312     0.008  \ngbc    0.7719  0.5952  0.6137     0.032  \nrf     0.7479  0.5691  0.5910     0.209  \nada    0.7543  0.5864  0.5970     0.038  \nnb     0.6709  0.4815  0.4964     0.007  \nknn    0.6559  0.4829  0.4957     0.047  \nqda    0.4319  0.2688  0.3032     0.008  \ndt     0.7265  0.5460  0.5579     0.007  \nsvm    0.8029  0.6464  0.6537     0.006  \nridge  0.8604  0.7507  0.7590     0.006  \n                                 Model  Accuracy     AUC  Recall   Prec.  \\\nlda       Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nlr                 Logistic Regression    0.8618  0.9557  0.8694  0.8362   \ngbc       Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nrf            Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \net              Extra Trees Classifier    0.7797  0.8659  0.6528  0.8024   \nada               Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nnb                         Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nknn             K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nqda    Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \ndt            Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nsvm                SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nridge                 Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \n\n           F1   Kappa     MCC  TT (Sec)  \nlda    0.8656  0.7609  0.7680     0.007  \nlr     0.8452  0.7206  0.7312     0.008  \ngbc    0.7719  0.5952  0.6137     0.032  \nrf     0.7479  0.5691  0.5910     0.209  \net     0.7003  0.5343  0.5540     0.193  \nada    0.7543  0.5864  0.5970     0.038  \nnb     0.6709  0.4815  0.4964     0.007  \nknn    0.6559  0.4829  0.4957     0.047  \nqda    0.4319  0.2688  0.3032     0.008  \ndt     0.7265  0.5460  0.5579     0.007  \nsvm    0.8029  0.6464  0.6537     0.006  \nridge  0.8604  0.7507  0.7590     0.006  \n                                   Model  Accuracy     AUC  Recall   Prec.  \\\nlda         Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nlr                   Logistic Regression    0.8618  0.9557  0.8694  0.8362   \ngbc         Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nxgboost        Extreme Gradient Boosting    0.8000  0.8861  0.7722  0.7809   \nrf              Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \net                Extra Trees Classifier    0.7797  0.8659  0.6528  0.8024   \nada                 Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nnb                           Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nknn               K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nqda      Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \ndt              Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nsvm                  SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nridge                   Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \n\n             F1   Kappa     MCC  TT (Sec)  \nlda      0.8656  0.7609  0.7680     0.007  \nlr       0.8452  0.7206  0.7312     0.008  \ngbc      0.7719  0.5952  0.6137     0.032  \nxgboost  0.7672  0.5919  0.6025     0.046  \nrf       0.7479  0.5691  0.5910     0.209  \net       0.7003  0.5343  0.5540     0.193  \nada      0.7543  0.5864  0.5970     0.038  \nnb       0.6709  0.4815  0.4964     0.007  \nknn      0.6559  0.4829  0.4957     0.047  \nqda      0.4319  0.2688  0.3032     0.008  \ndt       0.7265  0.5460  0.5579     0.007  \nsvm      0.8029  0.6464  0.6537     0.006  \nridge    0.8604  0.7507  0.7590     0.006  \n                                    Model  Accuracy     AUC  Recall   Prec.  \\\nlda          Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nlr                    Logistic Regression    0.8618  0.9557  0.8694  0.8362   \ngbc          Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nxgboost         Extreme Gradient Boosting    0.8000  0.8861  0.7722  0.7809   \nrf               Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \nlightgbm  Light Gradient Boosting Machine    0.7897  0.8789  0.7708  0.7661   \net                 Extra Trees Classifier    0.7797  0.8659  0.6528  0.8024   \nada                  Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nnb                            Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nknn                K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nqda       Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \ndt               Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nsvm                   SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nridge                    Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \n\n              F1   Kappa     MCC  TT (Sec)  \nlda       0.8656  0.7609  0.7680     0.007  \nlr        0.8452  0.7206  0.7312     0.008  \ngbc       0.7719  0.5952  0.6137     0.032  \nxgboost   0.7672  0.5919  0.6025     0.046  \nrf        0.7479  0.5691  0.5910     0.209  \nlightgbm  0.7525  0.5700  0.5865     0.013  \net        0.7003  0.5343  0.5540     0.193  \nada       0.7543  0.5864  0.5970     0.038  \nnb        0.6709  0.4815  0.4964     0.007  \nknn       0.6559  0.4829  0.4957     0.047  \nqda       0.4319  0.2688  0.3032     0.008  \ndt        0.7265  0.5460  0.5579     0.007  \nsvm       0.8029  0.6464  0.6537     0.006  \nridge     0.8604  0.7507  0.7590     0.006  \n                                    Model  Accuracy     AUC  Recall   Prec.  \\\nlda          Linear Discriminant Analysis    0.8824  0.9591  0.8819  0.8587   \nlr                    Logistic Regression    0.8618  0.9557  0.8694  0.8362   \ngbc          Gradient Boosting Classifier    0.8003  0.8867  0.7750  0.8018   \nxgboost         Extreme Gradient Boosting    0.8000  0.8861  0.7722  0.7809   \nrf               Random Forest Classifier    0.7895  0.8856  0.7486  0.7903   \nlightgbm  Light Gradient Boosting Machine    0.7897  0.8789  0.7708  0.7661   \net                 Extra Trees Classifier    0.7797  0.8659  0.6528  0.8024   \nada                  Ada Boost Classifier    0.8000  0.8522  0.7375  0.7921   \nnb                            Naive Bayes    0.7534  0.8437  0.6181  0.7633   \nknn                K Neighbors Classifier    0.7582  0.8307  0.6000  0.7663   \nqda       Quadratic Discriminant Analysis    0.6661  0.8169  0.4139  0.5399   \ndt               Decision Tree Classifier    0.7803  0.7714  0.7042  0.7774   \nsvm                   SVM - Linear Kernel    0.8255  0.0000  0.8208  0.7946   \nridge                    Ridge Classifier    0.8771  0.0000  0.8819  0.8498   \n\n              F1   Kappa     MCC  TT (Sec)  \nlda       0.8656  0.7609  0.7680     0.007  \nlr        0.8452  0.7206  0.7312     0.008  \ngbc       0.7719  0.5952  0.6137     0.032  \nxgboost   0.7672  0.5919  0.6025     0.046  \nrf        0.7479  0.5691  0.5910     0.209  \nlightgbm  0.7525  0.5700  0.5865     0.013  \net        0.7003  0.5343  0.5540     0.193  \nada       0.7543  0.5864  0.5970     0.038  \nnb        0.6709  0.4815  0.4964     0.007  \nknn       0.6559  0.4829  0.4957     0.047  \nqda       0.4319  0.2688  0.3032     0.008  \ndt        0.7265  0.5460  0.5579     0.007  \nsvm       0.8029  0.6464  0.6537     0.006  \nridge     0.8604  0.7507  0.7590     0.006  \n\n\ncreate and tune model\n\n\nmodel = create_model('lr')\n# 模型调参\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.8500  0.9688  0.7500  0.8571  0.8000  0.6809  0.6847\n1       0.8000  0.9697  0.8889  0.7273  0.8000  0.6040  0.6162\n2       0.9500  0.9798  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.8500  0.9394  0.7778  0.8750  0.8235  0.6939  0.6975\n4       0.8000  0.9495  0.8889  0.7273  0.8000  0.6040  0.6162\n5       0.8947  0.9545  1.0000  0.8000  0.8889  0.7912  0.8090\n6       0.8947  0.9886  0.8750  0.8750  0.8750  0.7841  0.7841\n7       0.7895  0.8636  0.8750  0.7000  0.7778  0.5824  0.5955\n8       0.8947  1.0000  1.0000  0.8000  0.8889  0.7912  0.8090\n9       0.8947  0.9432  0.7500  1.0000  0.8571  0.7765  0.7966\nMean    0.8618  0.9557  0.8694  0.8362  0.8452  0.7206  0.7312\nSD      0.0503  0.0360  0.0850  0.1010  0.0502  0.0986  0.0984\nmodel_tuned = tune_model(model, optimize = \"AUC\", n_iter = 10)\nFitting 10 folds for each of 10 candidates, totalling 100 fits\n      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n0       0.9000  0.9479  0.8750  0.8750  0.8750  0.7917  0.7917\n1       0.8500  0.9596  1.0000  0.7500  0.8571  0.7059  0.7385\n2       0.9500  0.9798  0.8889  1.0000  0.9412  0.8980  0.9027\n3       0.8500  0.9697  0.7778  0.8750  0.8235  0.6939  0.6975\n4       0.8000  0.9192  0.8889  0.7273  0.8000  0.6040  0.6162\n5       0.8421  0.9545  1.0000  0.7273  0.8421  0.6919  0.7273\n6       0.8421  0.9659  0.8750  0.7778  0.8235  0.6816  0.6854\n7       0.7895  0.8636  0.8750  0.7000  0.7778  0.5824  0.5955\n8       0.8947  0.9886  1.0000  0.8000  0.8889  0.7912  0.8090\n9       0.9474  0.9659  0.8750  1.0000  0.9333  0.8902  0.8956\nMean    0.8666  0.9515  0.9056  0.8232  0.8562  0.7331  0.7459\nSD      0.0525  0.0343  0.0688  0.1046  0.0512  0.1023  0.0994\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.4s finished\nmodel_tuned\nLogisticRegression(C=9.984, class_weight='balanced', dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\nextract data\n\n\n# 提取数据\ndata_x = get_config('X')\ndata_x.head()\n   Advertising  ShelveLoc_Medium  US_No  ...       Age  ShelveLoc_Good  Population\n0     1.122684               1.0    0.0  ... -1.716371             0.0   -1.248700\n1     0.664686               0.0    0.0  ...  0.036273             1.0   -1.657342\n2     1.733350               1.0    0.0  ... -0.464483             0.0    1.421098\n3     2.954680               1.0    0.0  ... -1.215616             0.0    0.828566\n4    -1.014644               1.0    1.0  ... -0.714860             0.0    1.598177\n\n[5 rows x 11 columns]\ndata_x.shape\n(280, 11)\n\n\nplot metrics\n\n\n# 指标查看 \nplot_model(best , plot='auc')\n# Hyperparameters         == parameter\n# AUC                     == auc\n# Confusion Matrix        == confusion_matrix\n# Threshold               == threshold\n# Precision Recall        == pr\n# Prediction Error        == error\n# Class Report            == class_report\n# Feature Selection       == rfe\n# Learning Curve          == learning\n# Manifold Learning       == manifold\n# Calibration Curve       == calibration\n# Validation Curve        == vc\n# Dimensions              == dimension\n# Feature Importance      == feature\n# Feature Importance All  == feature_all\n# Decision Boundary       == boundary\n# Lift Chart              == lift\n# Gain Chart              == gain\n# Decision Tree           == tree\n\n\n📌 model predict\nObtaining Transformed Unseen Data · Issue #893 · pycaret/pycaret\n\n\nunseen_data = test_data\nprep_pipe = get_config('prep_pipe')\ntransformed_unseen_data = prep_pipe.transform(unseen_data)\ntransformed_unseen_data.shape\n(120, 11)\n\n\n\n\nunseen_predictions = test_data\n\nunseen_predictions['Label'] = pd.Series(best.predict(transformed_unseen_data)) \n\nunseen_predictions['Score'] = pd.Series(best.predict_proba(transformed_unseen_data)[:,1])\n\n\n\n\n\npy$unseen_predictions <- \npy$unseen_predictions %>% \n  mutate(Label = as.character(Label))\n\n\n\n\n\n\nfrom pycaret.utils import check_metric\ncheck_metric(unseen_predictions['High'], unseen_predictions['Label'], metric = 'Accuracy')\n0.875\n\n\nmodel save\nmodel load\n\n\n\n",
    "preview": "posts/2021-06-17-islrtreebasedmethods/islrtreebasedmethods_photo/image-20210617171443567.png",
    "last_modified": "2021-06-17T17:19:13+08:00",
    "input_file": {},
    "preview_width": 1715,
    "preview_height": 948
  }
]
